---
title: Latest 15 Papers - December 24, 2025
labels: documentation
---
**Please check the [Github](https://github.com/zezhishao/MTS_Daily_ArXiv) page for a better reading experience and more papers.**

## reinforcement learning
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[WANDER: An Explainable Decision-Support Framework for HPC](https://arxiv.org/abs/2506.04049v2)** | 2025-12-22 |  |
| **[RAPID-LLM: Resilience-Aware Performance analysis of Infrastructure for Distributed LLM Training and Inference](https://arxiv.org/abs/2512.19606v1)** | 2025-12-22 | 11 pages, 12 figures |
| **[Parallel GPU-Enabled Algorithms for SpGEMM on Arbitrary Semirings with Hybrid Communication](https://arxiv.org/abs/2504.06408v2)** | 2025-12-22 |  |
| **[Faster Distributed Inference-Only Recommender Systems via Bounded Lag Synchronous Collectives](https://arxiv.org/abs/2512.19342v1)** | 2025-12-22 |  |
| **[Simulations between Strongly Sublinear MPC and Node-Capacitated Clique](https://arxiv.org/abs/2512.19326v1)** | 2025-12-22 |  |
| **[Dynamic Pricing for On-Demand DNN Inference in the Edge-AI Market](https://arxiv.org/abs/2503.04521v2)** | 2025-12-22 | <details><summary>Accep...</summary><p>Accepted for publication in IEEE Transactions on Mobile Computing. Index Terms: Edge-AI, DNN Inference Offloading, Resource Management, Dynamic Pricing, Auction Mechanism</p></details> |
| **[Small Language Models as Compiler Experts: Auto-Parallelization for Heterogeneous Systems](https://arxiv.org/abs/2512.19250v1)** | 2025-12-22 | <details><summary>Accep...</summary><p>Accepted at NeurIPS 2025 ML for Systems Workshop</p></details> |
| **[Efficiently Ranking Software Variants with Minimal Benchmarks](https://arxiv.org/abs/2509.06716v2)** | 2025-12-22 |  |
| **[L4: Low-Latency and Load-Balanced LLM Serving via Length-Aware Scheduling](https://arxiv.org/abs/2512.19179v1)** | 2025-12-22 | 15 pages, 16 figures |
| **[Evidential Trust-Aware Model Personalization in Decentralized Federated Learning for Wearable IoT](https://arxiv.org/abs/2512.19131v1)** | 2025-12-22 |  |
| **[Timely Parameter Updating in Over-the-Air Federated Learning](https://arxiv.org/abs/2512.19103v1)** | 2025-12-22 |  |
| **[Libra: Unleashing GPU Heterogeneity for High-Performance Sparse Matrix Multiplication](https://arxiv.org/abs/2506.22714v2)** | 2025-12-22 |  |
| **[JITServe: SLO-aware LLM Serving with Imprecise Request Information](https://arxiv.org/abs/2504.20068v3)** | 2025-12-22 |  |
| **[QoS-Aware Load Balancing in the Computing Continuum via Multi-Player Bandits](https://arxiv.org/abs/2512.18915v1)** | 2025-12-21 |  |
| **[A Real-Time Digital Twin for Adaptive Scheduling](https://arxiv.org/abs/2512.18894v1)** | 2025-12-21 | 5 pages, 3 figures |

## compiler
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[WANDER: An Explainable Decision-Support Framework for HPC](https://arxiv.org/abs/2506.04049v2)** | 2025-12-22 |  |
| **[RAPID-LLM: Resilience-Aware Performance analysis of Infrastructure for Distributed LLM Training and Inference](https://arxiv.org/abs/2512.19606v1)** | 2025-12-22 | 11 pages, 12 figures |
| **[Parallel GPU-Enabled Algorithms for SpGEMM on Arbitrary Semirings with Hybrid Communication](https://arxiv.org/abs/2504.06408v2)** | 2025-12-22 |  |
| **[Faster Distributed Inference-Only Recommender Systems via Bounded Lag Synchronous Collectives](https://arxiv.org/abs/2512.19342v1)** | 2025-12-22 |  |
| **[Simulations between Strongly Sublinear MPC and Node-Capacitated Clique](https://arxiv.org/abs/2512.19326v1)** | 2025-12-22 |  |
| **[Dynamic Pricing for On-Demand DNN Inference in the Edge-AI Market](https://arxiv.org/abs/2503.04521v2)** | 2025-12-22 | <details><summary>Accep...</summary><p>Accepted for publication in IEEE Transactions on Mobile Computing. Index Terms: Edge-AI, DNN Inference Offloading, Resource Management, Dynamic Pricing, Auction Mechanism</p></details> |
| **[Small Language Models as Compiler Experts: Auto-Parallelization for Heterogeneous Systems](https://arxiv.org/abs/2512.19250v1)** | 2025-12-22 | <details><summary>Accep...</summary><p>Accepted at NeurIPS 2025 ML for Systems Workshop</p></details> |
| **[Efficiently Ranking Software Variants with Minimal Benchmarks](https://arxiv.org/abs/2509.06716v2)** | 2025-12-22 |  |
| **[L4: Low-Latency and Load-Balanced LLM Serving via Length-Aware Scheduling](https://arxiv.org/abs/2512.19179v1)** | 2025-12-22 | 15 pages, 16 figures |
| **[Evidential Trust-Aware Model Personalization in Decentralized Federated Learning for Wearable IoT](https://arxiv.org/abs/2512.19131v1)** | 2025-12-22 |  |
| **[Timely Parameter Updating in Over-the-Air Federated Learning](https://arxiv.org/abs/2512.19103v1)** | 2025-12-22 |  |
| **[Libra: Unleashing GPU Heterogeneity for High-Performance Sparse Matrix Multiplication](https://arxiv.org/abs/2506.22714v2)** | 2025-12-22 |  |
| **[JITServe: SLO-aware LLM Serving with Imprecise Request Information](https://arxiv.org/abs/2504.20068v3)** | 2025-12-22 |  |
| **[QoS-Aware Load Balancing in the Computing Continuum via Multi-Player Bandits](https://arxiv.org/abs/2512.18915v1)** | 2025-12-21 |  |
| **[A Real-Time Digital Twin for Adaptive Scheduling](https://arxiv.org/abs/2512.18894v1)** | 2025-12-21 | 5 pages, 3 figures |

## performance
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[WANDER: An Explainable Decision-Support Framework for HPC](https://arxiv.org/abs/2506.04049v2)** | 2025-12-22 |  |
| **[RAPID-LLM: Resilience-Aware Performance analysis of Infrastructure for Distributed LLM Training and Inference](https://arxiv.org/abs/2512.19606v1)** | 2025-12-22 | 11 pages, 12 figures |
| **[Parallel GPU-Enabled Algorithms for SpGEMM on Arbitrary Semirings with Hybrid Communication](https://arxiv.org/abs/2504.06408v2)** | 2025-12-22 |  |
| **[Faster Distributed Inference-Only Recommender Systems via Bounded Lag Synchronous Collectives](https://arxiv.org/abs/2512.19342v1)** | 2025-12-22 |  |
| **[Simulations between Strongly Sublinear MPC and Node-Capacitated Clique](https://arxiv.org/abs/2512.19326v1)** | 2025-12-22 |  |
| **[Dynamic Pricing for On-Demand DNN Inference in the Edge-AI Market](https://arxiv.org/abs/2503.04521v2)** | 2025-12-22 | <details><summary>Accep...</summary><p>Accepted for publication in IEEE Transactions on Mobile Computing. Index Terms: Edge-AI, DNN Inference Offloading, Resource Management, Dynamic Pricing, Auction Mechanism</p></details> |
| **[Small Language Models as Compiler Experts: Auto-Parallelization for Heterogeneous Systems](https://arxiv.org/abs/2512.19250v1)** | 2025-12-22 | <details><summary>Accep...</summary><p>Accepted at NeurIPS 2025 ML for Systems Workshop</p></details> |
| **[Efficiently Ranking Software Variants with Minimal Benchmarks](https://arxiv.org/abs/2509.06716v2)** | 2025-12-22 |  |
| **[L4: Low-Latency and Load-Balanced LLM Serving via Length-Aware Scheduling](https://arxiv.org/abs/2512.19179v1)** | 2025-12-22 | 15 pages, 16 figures |
| **[Evidential Trust-Aware Model Personalization in Decentralized Federated Learning for Wearable IoT](https://arxiv.org/abs/2512.19131v1)** | 2025-12-22 |  |
| **[Timely Parameter Updating in Over-the-Air Federated Learning](https://arxiv.org/abs/2512.19103v1)** | 2025-12-22 |  |
| **[Libra: Unleashing GPU Heterogeneity for High-Performance Sparse Matrix Multiplication](https://arxiv.org/abs/2506.22714v2)** | 2025-12-22 |  |
| **[JITServe: SLO-aware LLM Serving with Imprecise Request Information](https://arxiv.org/abs/2504.20068v3)** | 2025-12-22 |  |
| **[QoS-Aware Load Balancing in the Computing Continuum via Multi-Player Bandits](https://arxiv.org/abs/2512.18915v1)** | 2025-12-21 |  |
| **[A Real-Time Digital Twin for Adaptive Scheduling](https://arxiv.org/abs/2512.18894v1)** | 2025-12-21 | 5 pages, 3 figures |

