# Daily Papers
The project automatically fetches the latest papers from arXiv based on keywords.

The subheadings in the README file represent the search keywords.

Only the most recent articles for each keyword are retained, up to a maximum of 100 papers.

You can click the 'Watch' button to receive daily email notifications.

Last update: 2026-02-18

## reinforcement learning
| **Title** | **Date** | **Abstract** | **Comment** |
| --- | --- | --- | --- |
| **[Stable-MoE: Lyapunov-based Token Routing for Distributed Mixture-of-Experts Training over Edge Networks](https://arxiv.org/abs/2512.06784v2)** | 2026-02-16 | <details><summary>Show</summary><p>The sparse activation mechanism of mixture of experts (MoE) model empowers edge intelligence with enhanced training efficiency and reduced computational resource consumption. However, traditional token routing in distributed MoE training faces significant challenges in resource-constrained edge networks characterized by heterogeneous computing capabilities and stochastic token arrivals, which inevitably suffer from workload backlog, resource inefficiency, and performance degradation. To address this issue, we propose a novel Lyapunov-based token routing framework for distributed MoE training over resource-heterogeneous edge networks, termed Stable-MoE. Specifically, we formulate a stochastic optimization problem to maximize both system throughput and gating consistency via optimizing the token routing strategy and computational resource allocation, while ensuring long-term stability of both token and energy queues at the edge devices. Using the Lyapunov optimization, we transform the intractable long-term optimization problem into tractable per-slot subproblems by enabling online decision-making of token routing and computation frequency utilization without the knowledge of future system states. Experimental results on the SVHN and CIFAR-100 datasets demonstrate that Stable-MoE outperforms the baselines with at least 40% and 5% gains in system throughput and test accuracy, respectively.</p></details> |  |
| **[Transforming Lock-free Linked Lists into Distributed Lock-free Linked Lists](https://arxiv.org/abs/2510.06387v2)** | 2026-02-16 | <details><summary>Show</summary><p>Modern databases use dynamic search structures that store an enormous amount of data, and often serve them using multi-threaded algorithms to support the ever-increasing throughput needs. When this throughput need exceeds the capacity of the machine hosting the structure, one either needs to replace the underlying hardware (an option that is typically not viable and introduces a long down time) or make the data structure distributed. Static partitioning of the data structure for distribution is not desirable, as it is prone to uneven load distribution over time, and having to change the partitioning scheme later will require downtime. The goal of this paper is to extend a concurrent data structure to distributed data structures that provide dynamic load balancing while preserving important properties such as lock freedom. With this intuition, first, we introduce the notion of conditional lock-freedom that extends the notion of lock-free computation with reasonable assumptions about communication between processes. Then, we present DiLi, a conditional lock-free, linearizable, and distributable linked list that can be asynchronously and dynamically (1) partitioned into multiple sublists and (2) load balanced by distributing sublists across multiple machines. DiLi contains primitives for these that also maintain the lock-free property of the underlying search structure that supports find, remove, and insert of a key as the client operations. We show that DiLi bridges the gap between concurrent data structures and distributed data structures. Specifically, DiLi provides comparable (and better in write-intensive workloads) performance to skip lists (which are typically the fastest data structures for search in a concurrent environment). Plus, it provides horizontal scaling by permitting dynamic load balancing on a distributed environment.</p></details> |  |
| **[Low-Latency Federated Fine-Tuning for Large Language Models Over Wireless Networks](https://arxiv.org/abs/2602.01024v2)** | 2026-02-16 | <details><summary>Show</summary><p>Recently, federated large language models (LLMs) have drawn significant attention thanks to coupled capabilities of LLMs and federated learning (FL) that address privacy concerns in collaborative fine-tuning. However, due to large-scale parameters of LLMs, existing federated LLM fine-tuning frameworks incur significant challenges in resource-constrained clients characterized by heterogeneous computing capabilities and random wireless channels. To address this issue, we propose a joint client-specific pruning and bandwidth allocation (JCPBA) framework for federated LLMs to improve the fine-tuning efficiency over the wireless networks. Specifically, we formulate a fine-tuning latency minimization problem by jointly optimizing pruning rates and bandwidth allocations. Furthermore, we solve this optimization problem using a block coordinate descent method. Extensive experiments on the datasets of Yahoo Answers and GSM8K demonstrate that the proposed framework significantly reduces wall-clock fine-tuning time compared with state-of-the-art baselines and gains equal or lower test loss at the cost of lower computation and communication overhead.</p></details> |  |
| **[EdgeFaaS: A Function-based Framework for Edge Computing](https://arxiv.org/abs/2210.01410v2)** | 2026-02-16 | <details><summary>Show</summary><p>The rapid growth of data generated from Internet of Things (IoTs) such as smart phones and smart home devices presents new challenges to cloud computing in transferring, storing, and processing the data. With increasingly more powerful edge devices, edge computing, on the other hand, has the potential to better responsiveness, privacy, and cost efficiency. However, resources across the cloud and edge are highly distributed and highly diverse. To address these challenges, this paper proposes EdgeFaaS, a Function-as-a-Service (FaaS) based computing framework that supports the flexible, convenient, and optimized use of distributed and heterogeneous resources across IoT, edge, and cloud systems. EdgeFaaS allows cluster resources and individual devices to be managed under the same framework and provide computational and storage resources for functions. It provides virtual function and virtual storage interfaces for consistent function management and storage management across heterogeneous compute and storage resources. It automatically optimizes the scheduling of functions and placement of data according to their performance and privacy requirements. EdgeFaaS is evaluated based on two edge workflows: video analytics workflow and federated learning workflow, both of which are representative edge applications and involve large amounts of input data generated from edge devices.</p></details> | <details><summary>This ...</summary><p>This paper has been withdrawn by the authors because it was submitted without the final consent of all co-authors</p></details> |
| **[Atomix: Timely, Transactional Tool Use for Reliable Agentic Workflows](https://arxiv.org/abs/2602.14849v1)** | 2026-02-16 | <details><summary>Show</summary><p>LLM agents increasingly act on external systems, yet tool effects are immediate. Under failures, speculation, or contention, losing branches can leak unintended side effects with no safe rollback. We introduce Atomix, a runtime that provides progress-aware transactional semantics for agent tool calls. Atomix tags each call with an epoch, tracks per-resource frontiers, and commits only when progress predicates indicate safety; bufferable effects can be delayed, while externalized effects are tracked and compensated on abort. Across real workloads with fault injection, transactional retry improves task success, while frontier-gated commit strengthens isolation under speculation and contention.</p></details> |  |
| **[Nightjar: Dynamic Adaptive Speculative Decoding for Large Language Models Serving](https://arxiv.org/abs/2512.22420v2)** | 2026-02-16 | <details><summary>Show</summary><p>Speculative decoding (SD) accelerates LLM inference by verifying draft tokens in parallel. However, this method presents a critical trade-off: it improves throughput in low-load, memory-bound systems but degrades performance in high-load, compute-bound environments due to verification overhead. Current SD implementations use a fixed speculative length, failing to adapt to dynamic request rates and creating a significant performance bottleneck in real-world serving scenarios. To overcome this, we propose Nightjar, a novel learning-based algorithm for adaptive speculative inference that adjusts to request load by dynamically selecting the optimal speculative length for different batch sizes and even disabling speculative decoding when it provides no benefit. Experiments show that Nightjar achieves up to 14.8% higher throughput and 20.2% lower latency compared to standard speculative decoding, demonstrating robust efficiency for real-time serving.</p></details> | 6 pages, 11 figures |
| **[Optimal Program Synthesis via Abstract Interpretation](https://arxiv.org/abs/2602.14717v1)** | 2026-02-16 | <details><summary>Show</summary><p>We consider the problem of synthesizing programs with numerical constants that optimize a quantitative objective, such as accuracy, over a set of input-output examples. We propose a general framework for optimal synthesis of such programs in a given domain specific language (DSL), with provable optimality guarantees. Our framework enumerates programs in a general search graph, where nodes represent subsets of concrete programs. To improve scalability, it uses A* search in conjunction with a search heuristic based on abstract interpretation; intuitively, this heuristic establishes upper bounds on the value of subtrees in the search graph, enabling the synthesizer to identify and prune subtrees that are provably suboptimal. In addition, we propose a natural strategy for constructing abstract transformers for monotonic semantics, which is a common property for components in DSLs for data classification. Finally, we implement our approach in the context of two such existing DSLs, demonstrating that our algorithm is more scalable than existing optimal synthesizers.</p></details> |  |
| **[Evaluation of Dynamic Vector Bin Packing for Virtual Machine Placement](https://arxiv.org/abs/2602.14704v1)** | 2026-02-16 | <details><summary>Show</summary><p>Virtual machine placement is a crucial challenge in cloud computing for efficiently utilizing physical machine resources in data centers. Virtual machine placement can be formulated as a MinUsageTime Dynamic Vector Bin Packing (DVBP) problem, aiming to minimize the total usage time of the physical machines. This paper evaluates state-of-the-art MinUsageTime DVBP algorithms in non-clairvoyant, clairvoyant and learning-augmented online settings, where item durations (virtual machine lifetimes) are unknown, known and predicted, respectively. Besides the algorithms taken from the literature, we also develop several new algorithms or enhancements. Empirical experimentation is carried out with real-world datasets of Microsoft Azure. The insights from the experimental results are discussed to explore the structures of algorithms and promising design elements that work well in practice.</p></details> | <details><summary>Exten...</summary><p>Extended version of a paper that will appear in IEEE IPDPS 2026 conference</p></details> |
| **[LCLs Beyond Bounded Degrees](https://arxiv.org/abs/2602.02340v2)** | 2026-02-16 | <details><summary>Show</summary><p>The study of Locally Checkable Labelings (LCLs) has led to a remarkably precise characterization of the distributed time complexities that can occur on bounded-degree trees. A central feature of this complexity landscape is the existence of gap results, which rule out large ranges of intermediate complexities. While it was initially hoped that these gaps might extend to more general graph classes, this has turned out not to be the case. In this work, we investigate a different direction: we remain in the class of trees, but allow arbitrarily large degrees. We focus on the \emph{polynomial regime} ($Θ(n^{1/k} \mid k \in \mathbb{N})$) and show that whether polynomial gap results persist in the unbounded-degree setting depends on how LCLs are generalized beyond bounded degrees. There already exists a complex construction that shows that the polynomial gaps also vanish for LCLs on unbounded degree trees. Rather than stopping at this negative result, we give a much simpler set of problems that already contradicts the existence of any polynomial gaps. The insight obtained from this cleaner construction is that for gap results to exist, we cannot allow problems to distinguish infinitely many cases. This guides us to a natural class of problems for which polynomial gap results can still be recovered. We introduce \emph{Locally Finite Labelings} (LFLs), which formalize the intuition that \emph{every node must fall into one of finitely many local cases}. Our main result shows that this restriction is sufficient to restore the polynomial gaps: for any LFL $Π$ on trees with unbounded degrees, the deterministic LOCAL complexity of $Π$ is either - $Θ(n^{1/k})$ for some integer $k \geq 1$, or - $O(\log n)$. Moreover, which case applies, and the corresponding value of $k$, can be determined solely from the description of $Π$.</p></details> |  |
| **[Leader Election via Unique Sink Orientation](https://arxiv.org/abs/2511.19208v2)** | 2026-02-16 | <details><summary>Show</summary><p>A Locally Checkable Labeling (LCL) is a specification describing a set of labels that are valid with respect to a set of conditions that characterize a local part of a solution to a global problem. Conditions can only refer to nodes and labels within a constant radius neighborhood of each node. This work studies local labeling schemes whose global consistency implies solutions to two classical problems: leader election and spanning tree construction. For each problem, we present a local labeling scheme using one bit per edge or equivalently $O(Δ)$ bits per node (where $Δ$ is the maximum degree in the graph), with conditions checkable within the graph induced by the one neighborhood of each node. For leader election, we show that global satisfaction of the conditions implies the existence of a unique sink in the graph, which we define to be a leader, while in the spanning tree setting it implies that a specific subset of edges induces a spanning tree rooted at a given node. We show those implications for $K_4$-free dismantlable and chordal graphs in the former case and for dismantlable graphs in the latter, assuming a root is given. For chordal graphs, the labeling implying a unique sink additionally induces an acyclic orientation. This property is not generally locally verifiable with constant-size labels in arbitrary graphs. To the best of our knowledge, these are the first local labeling results tailored to ($K_4$-free) dismantlable graphs, potentially highlighting structural properties useful for designing LCLs for additional problems. Finally, we present a generic transformation that converts any local labeling scheme into a silent self-stabilizing algorithm by adding only one extra state, assuming a Gouda fair scheduler. This transformation may be of independent interest.</p></details> | <details><summary>19 pa...</summary><p>19 pages, 4 figures, under review</p></details> |
| **[Understanding GPU Resource Interference One Level Deeper](https://arxiv.org/abs/2501.16909v3)** | 2026-02-16 | <details><summary>Show</summary><p>GPUs are vastly underutilized, even when running resource-intensive AI applications, as GPU kernels within each job have diverse resource profiles that may saturate some parts of a device while often leaving other parts idle. Colocating applications is known to improve GPU utilization, but is not common practice as it becomes difficult to provide predictable performance due to workload interference. Providing predictable performance guarantees requires a deep understanding of how applications contend for shared GPU resources such as block schedulers, compute units, L1/L2 caches, and memory bandwidth. We study the key types of GPU resource interference and develop a methodology to quantify the sensitivity of a workload to each type. We discuss how this methodology can serve as the foundation for GPU schedulers that enforce strict performance guarantees and how application developers can design GPU kernels with colocation in mind to improve efficiency.</p></details> |  |
| **[Polar: An Algebraic Analyzer for (Probabilistic) Loops](https://arxiv.org/abs/2602.14573v1)** | 2026-02-16 | <details><summary>Show</summary><p>We present the Polar framework for fully automating the analysis of classical and probabilistic loops using algebraic reasoning. The central theme in Polar comes with handling algebraic recurrences that precisely capture the loop semantics. To this end, our work implements a variety of techniques to compute exact closed-forms of recurrences over higher-order moments of variables, infer invariants, and derive loop sensitivities with respect to unknown parameters. Polar can analyze probabilistic loops containing if-statements, polynomial arithmetic, and common probability distributions. By translating loop analysis into linear recurrence solving, Polar uses the derived closed-forms of recurrences to compute the strongest polynomial invariant or to infer parameter sensitivity. Polar is both sound and complete within well-defined programming model restrictions. Lifting any of these restrictions results in significant hardness limits of computation. To overcome computational burdens for the sake of efficiency, Polar also provides incomplete but sound techniques to compute moments of combinations of variables.</p></details> | <details><summary>Publi...</summary><p>Published in "Principles of Verification: Cycling the Probabilistic Landscape"</p></details> |
| **[Efficient Multi-round LLM Inference over Disaggregated Serving](https://arxiv.org/abs/2602.14516v1)** | 2026-02-16 | <details><summary>Show</summary><p>With the rapid evolution of Large Language Models (LLMs), multi-round workflows, such as autonomous agents and iterative retrieval, have become increasingly prevalent. However, this raises hurdles for serving LLMs under prefill-decode (PD) disaggregation, a widely adopted paradigm that separates the compute-bound prefill phase and memory-bound decode phase onto individual resources. Specifically, existing systems overlook the interleaved prefill-decode workload pattern in multi-round inference, leading to sub-optimal handling of the incremental prefill workloads and model deployment for the two phases. In this work, we present AMPD, a brand new disaggregated serving framework for multi-round LLM inference. The core of AMPD is to coordinate the prefill workloads based on real-time workloads by adaptively determining where to carry out these workloads and how they are scheduled, in order to maximize service level objective (SLO) attainment. In addition, we tailor a planning algorithm for our scenario, facilitating the deduction of optimal resource allocation and parallel strategies for the two phases. Empirical results demonstrate that AMPD substantially improves SLO attainment compared to state-of-the-art baselines.</p></details> |  |
| **[Evidential Trust-Aware Model Personalization in Decentralized Federated Learning for Wearable IoT](https://arxiv.org/abs/2512.19131v2)** | 2026-02-16 | <details><summary>Show</summary><p>Decentralized federated learning (DFL) enables collaborative model training across edge devices without centralized coordination, offering resilience against single points of failure. However, statistical heterogeneity arising from non-identically distributed local data creates a fundamental challenge: nodes must learn personalized models adapted to their local distributions while selectively collaborating with compatible peers. Existing approaches either enforce a single global model that fits no one well, or rely on heuristic peer selection mechanisms that cannot distinguish between peers with genuinely incompatible data distributions and those with valuable complementary knowledge. We present Murmura, a framework that leverages evidential deep learning to enable trust-aware model personalization in DFL. Our key insight is that epistemic uncertainty from Dirichlet-based evidential models directly indicates peer compatibility: high epistemic uncertainty when a peer's model evaluates local data reveals distributional mismatch, enabling nodes to exclude incompatible influence while maintaining personalized models through selective collaboration. Murmura introduces a trust-aware aggregation mechanism that computes peer compatibility scores through cross-evaluation on local validation samples and personalizes model aggregation based on evidential trust with adaptive thresholds. Evaluation on three wearable IoT datasets (UCI HAR, PAMAP2, PPG-DaLiA) demonstrates that Murmura reduces performance degradation from IID to non-IID conditions compared to baseline (0.9% vs. 19.3%), achieves 7.4$\times$ faster convergence, and maintains stable accuracy across hyperparameter choices. These results establish evidential uncertainty as a principled foundation for compatibility-aware personalization in decentralized heterogeneous environments.</p></details> | <details><summary>v2. A...</summary><p>v2. Addressed minor reviewer concerns</p></details> |
| **[Floe: Federated Specialization for Real-Time LLM-SLM Inference](https://arxiv.org/abs/2602.14302v1)** | 2026-02-15 | <details><summary>Show</summary><p>Deploying large language models (LLMs) in real-time systems remains challenging due to their substantial computational demands and privacy concerns. We propose Floe, a hybrid federated learning framework designed for latency-sensitive, resource-constrained environments. Floe combines a cloud-based black-box LLM with lightweight small language models (SLMs) on edge devices to enable low-latency, privacy-preserving inference. Personal data and fine-tuning remain on-device, while the cloud LLM contributes general knowledge without exposing proprietary weights. A heterogeneity-aware LoRA adaptation strategy enables efficient edge deployment across diverse hardware, and a logit-level fusion mechanism enables real-time coordination between edge and cloud models. Extensive experiments demonstrate that Floe enhances user privacy and personalization. Moreover, it significantly improves model performance and reduces inference latency on edge devices under real-time constraints compared with baseline approaches.</p></details> | <details><summary>Accep...</summary><p>Accepted by IEEE Transactions on Parallel and Distributed Systems</p></details> |
| **[Parallel Sparse and Data-Sparse Factorization-based Linear Solvers](https://arxiv.org/abs/2602.14289v1)** | 2026-02-15 | <details><summary>Show</summary><p>Efficient solutions of large-scale, ill-conditioned and indefinite algebraic equations are ubiquitously needed in numerous computational fields, including multiphysics simulations, machine learning, and data science. Because of their robustness and accuracy, direct solvers are crucial components in building a scalable solver toolchain. In this article, we will review recent advances of sparse direct solvers along two axes: 1) reducing communication and latency costs in both task- and data-parallel settings, and 2) reducing computational complexity via low-rank and other compression techniques such as hierarchical matrix algebra. In addition to algorithmic principles, we also illustrate the key parallelization challenges and best practices to deliver high speed and reliability on modern heterogeneous parallel machines.</p></details> |  |
| **[SlimEdge: Performance and Device Aware Distributed DNN Deployment on Resource-Constrained Edge Hardware](https://arxiv.org/abs/2512.22136v2)** | 2026-02-15 | <details><summary>Show</summary><p>Distributed deep neural networks (DNNs) have become central to modern computer vision, yet their deployment on resource-constrained edge devices remains hindered by substantial parameter counts, computational demands, and the probability of device failure. Here, we present an approach to the efficient deployment of distributed DNNs that jointly respect hardware limitations, preserve task performance, and remain robust to partial system failures. Our method integrates structured model pruning with a multi-objective optimization framework to tailor network capacity for heterogeneous device constraints, while explicitly accounting for device availability and failure probability during deployment. We demonstrate this framework using Multi-View Convolutional Neural Networks (MVCNN), a state-of-the-art architecture for 3D object recognition, by quantifying the contribution of individual views to classification accuracy and allocating pruning budgets accordingly. Experimental results show that the resulting models satisfy user-specified bounds on accuracy and memory footprint, even under multiple simultaneous device failures. The inference time is reduced by factors up to 4.7x across diverse simulated device configurations. These findings suggest that performance-aware, view-adaptive, and failure-resilient compression provides a viable pathway for deploying complex vision models in distributed edge environments.</p></details> |  |
| **[Dual-Signal Adaptive KV-Cache Optimization for Long-Form Video Understanding in Vision-Language Models](https://arxiv.org/abs/2602.14236v1)** | 2026-02-15 | <details><summary>Show</summary><p>Vision-Language Models (VLMs) face a critical memory bottleneck when processing long-form video content due to the linear growth of the Key-Value (KV) cache with sequence length. Existing solutions predominantly employ reactive eviction strategies that compute full attention matrices before discarding tokens, resulting in substantial computational waste. We propose Sali-Cache, a novel a priori optimization framework that implements dual-signal adaptive caching through proactive memory management. By integrating a temporal filter based on optical flow analysis for detecting inter-frame redundancy and a spatial filter leveraging saliency detection for identifying visually significant regions, Sali-Cache intelligently manages memory allocation before entering computationally expensive attention operations. Experimental evaluation on the LLaVA 1.6 architecture demonstrates that our method achieves a 2.20x compression ratio in effective memory usage while maintaining 100% accuracy across BLEU, ROUGE-L, and Exact Match metrics. Furthermore, under identical memory budget constraints, Sali-Cache preserves context-rich features over extended temporal durations without degrading model performance, enabling efficient processing of long-form video content on consumer-grade hardware.</p></details> |  |
| **[Optimizing Task Scheduling in Fog Computing with Deadline Awareness](https://arxiv.org/abs/2509.07378v5)** | 2026-02-15 | <details><summary>Show</summary><p>The rise of Internet of Things (IoT) devices has led to the development of numerous time-sensitive applications that require quick responses and low latency. Fog computing has emerged as a solution for processing these IoT applications, but it faces challenges such as resource allocation and job scheduling. Therefore, it is crucial to determine how to assign and schedule tasks on Fog nodes. This work aims to schedule tasks in IoT while minimizing the total energy consumption of nodes and enhancing the Quality of Service (QoS) requirements of IoT tasks, taking into account task deadlines. This paper classifies Fog nodes into two categories based on their traffic level: low and high. It schedules short-deadline tasks on low-traffic nodes using an Improved Golden Eagle Optimization (IGEO) algorithm, an enhancement that utilizes genetic operators for discretization. Long-deadline tasks are processed on high-traffic nodes using reinforcement learning (RL). This combined approach is called the Reinforcement Improved Golden Eagle Optimization (RIGEO) algorithm. Experimental results demonstrate that RIGEO achieves up to a 29% reduction in energy consumption, up to an 86% improvement in response time, and up to a 19% reduction in deadline violations compared to state-of-the-art algorithms.</p></details> |  |
| **[ML-ECS: A Collaborative Multimodal Learning Framework for Edge-Cloud Synergies](https://arxiv.org/abs/2602.14107v1)** | 2026-02-15 | <details><summary>Show</summary><p>Edge-cloud synergies provide a promising paradigm for privacy-preserving deployment of foundation models, where lightweight on-device models adapt to domain-specific data and cloud-hosted models coordinate knowledge sharing. However, in real-world edge environments, collaborative multimodal learning is challenged by modality heterogeneity (different modality combinations across domains) and model-structure heterogeneity (different modality-specific encoders/fusion modules. To address these issues, we propose ML-ECS, a collaborative multimodal learning framework that enables joint training between a server-based model and heterogeneous edge models. This framework consists of four components: (1) cross-modal contrastive learning (CCL) to align modality representations in a shared latent space, (2) adaptive multimodal tuning (AMT) to preserve domain-specific knowledge from local datasets, (3) modality-aware model aggregation (MMA) to robustly aggregate while mitigating noise caused by missing modalities, and (4) SLM-enhanced CCL (SE-CCL) to facilitate bidirectional knowledge transfer between cloud and edge. Experimental results on various multimodal tasks show that \pname consistently outperform state-of-the-art baselines under varying modality availability, achieving improvements of 5.44% to 12.08% in Rouge-LSum and improving both client- and server-side performance. In addition, by communicating only low-rank LoRA parameters and fused representations, ML-ECS achieves high communication efficiency, requiring only 0.65% of the total parameter volume.</p></details> |  |

## compiler
| **Title** | **Date** | **Abstract** | **Comment** |
| --- | --- | --- | --- |
| **[Stable-MoE: Lyapunov-based Token Routing for Distributed Mixture-of-Experts Training over Edge Networks](https://arxiv.org/abs/2512.06784v2)** | 2026-02-16 | <details><summary>Show</summary><p>The sparse activation mechanism of mixture of experts (MoE) model empowers edge intelligence with enhanced training efficiency and reduced computational resource consumption. However, traditional token routing in distributed MoE training faces significant challenges in resource-constrained edge networks characterized by heterogeneous computing capabilities and stochastic token arrivals, which inevitably suffer from workload backlog, resource inefficiency, and performance degradation. To address this issue, we propose a novel Lyapunov-based token routing framework for distributed MoE training over resource-heterogeneous edge networks, termed Stable-MoE. Specifically, we formulate a stochastic optimization problem to maximize both system throughput and gating consistency via optimizing the token routing strategy and computational resource allocation, while ensuring long-term stability of both token and energy queues at the edge devices. Using the Lyapunov optimization, we transform the intractable long-term optimization problem into tractable per-slot subproblems by enabling online decision-making of token routing and computation frequency utilization without the knowledge of future system states. Experimental results on the SVHN and CIFAR-100 datasets demonstrate that Stable-MoE outperforms the baselines with at least 40% and 5% gains in system throughput and test accuracy, respectively.</p></details> |  |
| **[Transforming Lock-free Linked Lists into Distributed Lock-free Linked Lists](https://arxiv.org/abs/2510.06387v2)** | 2026-02-16 | <details><summary>Show</summary><p>Modern databases use dynamic search structures that store an enormous amount of data, and often serve them using multi-threaded algorithms to support the ever-increasing throughput needs. When this throughput need exceeds the capacity of the machine hosting the structure, one either needs to replace the underlying hardware (an option that is typically not viable and introduces a long down time) or make the data structure distributed. Static partitioning of the data structure for distribution is not desirable, as it is prone to uneven load distribution over time, and having to change the partitioning scheme later will require downtime. The goal of this paper is to extend a concurrent data structure to distributed data structures that provide dynamic load balancing while preserving important properties such as lock freedom. With this intuition, first, we introduce the notion of conditional lock-freedom that extends the notion of lock-free computation with reasonable assumptions about communication between processes. Then, we present DiLi, a conditional lock-free, linearizable, and distributable linked list that can be asynchronously and dynamically (1) partitioned into multiple sublists and (2) load balanced by distributing sublists across multiple machines. DiLi contains primitives for these that also maintain the lock-free property of the underlying search structure that supports find, remove, and insert of a key as the client operations. We show that DiLi bridges the gap between concurrent data structures and distributed data structures. Specifically, DiLi provides comparable (and better in write-intensive workloads) performance to skip lists (which are typically the fastest data structures for search in a concurrent environment). Plus, it provides horizontal scaling by permitting dynamic load balancing on a distributed environment.</p></details> |  |
| **[Low-Latency Federated Fine-Tuning for Large Language Models Over Wireless Networks](https://arxiv.org/abs/2602.01024v2)** | 2026-02-16 | <details><summary>Show</summary><p>Recently, federated large language models (LLMs) have drawn significant attention thanks to coupled capabilities of LLMs and federated learning (FL) that address privacy concerns in collaborative fine-tuning. However, due to large-scale parameters of LLMs, existing federated LLM fine-tuning frameworks incur significant challenges in resource-constrained clients characterized by heterogeneous computing capabilities and random wireless channels. To address this issue, we propose a joint client-specific pruning and bandwidth allocation (JCPBA) framework for federated LLMs to improve the fine-tuning efficiency over the wireless networks. Specifically, we formulate a fine-tuning latency minimization problem by jointly optimizing pruning rates and bandwidth allocations. Furthermore, we solve this optimization problem using a block coordinate descent method. Extensive experiments on the datasets of Yahoo Answers and GSM8K demonstrate that the proposed framework significantly reduces wall-clock fine-tuning time compared with state-of-the-art baselines and gains equal or lower test loss at the cost of lower computation and communication overhead.</p></details> |  |
| **[EdgeFaaS: A Function-based Framework for Edge Computing](https://arxiv.org/abs/2210.01410v2)** | 2026-02-16 | <details><summary>Show</summary><p>The rapid growth of data generated from Internet of Things (IoTs) such as smart phones and smart home devices presents new challenges to cloud computing in transferring, storing, and processing the data. With increasingly more powerful edge devices, edge computing, on the other hand, has the potential to better responsiveness, privacy, and cost efficiency. However, resources across the cloud and edge are highly distributed and highly diverse. To address these challenges, this paper proposes EdgeFaaS, a Function-as-a-Service (FaaS) based computing framework that supports the flexible, convenient, and optimized use of distributed and heterogeneous resources across IoT, edge, and cloud systems. EdgeFaaS allows cluster resources and individual devices to be managed under the same framework and provide computational and storage resources for functions. It provides virtual function and virtual storage interfaces for consistent function management and storage management across heterogeneous compute and storage resources. It automatically optimizes the scheduling of functions and placement of data according to their performance and privacy requirements. EdgeFaaS is evaluated based on two edge workflows: video analytics workflow and federated learning workflow, both of which are representative edge applications and involve large amounts of input data generated from edge devices.</p></details> | <details><summary>This ...</summary><p>This paper has been withdrawn by the authors because it was submitted without the final consent of all co-authors</p></details> |
| **[Atomix: Timely, Transactional Tool Use for Reliable Agentic Workflows](https://arxiv.org/abs/2602.14849v1)** | 2026-02-16 | <details><summary>Show</summary><p>LLM agents increasingly act on external systems, yet tool effects are immediate. Under failures, speculation, or contention, losing branches can leak unintended side effects with no safe rollback. We introduce Atomix, a runtime that provides progress-aware transactional semantics for agent tool calls. Atomix tags each call with an epoch, tracks per-resource frontiers, and commits only when progress predicates indicate safety; bufferable effects can be delayed, while externalized effects are tracked and compensated on abort. Across real workloads with fault injection, transactional retry improves task success, while frontier-gated commit strengthens isolation under speculation and contention.</p></details> |  |
| **[Nightjar: Dynamic Adaptive Speculative Decoding for Large Language Models Serving](https://arxiv.org/abs/2512.22420v2)** | 2026-02-16 | <details><summary>Show</summary><p>Speculative decoding (SD) accelerates LLM inference by verifying draft tokens in parallel. However, this method presents a critical trade-off: it improves throughput in low-load, memory-bound systems but degrades performance in high-load, compute-bound environments due to verification overhead. Current SD implementations use a fixed speculative length, failing to adapt to dynamic request rates and creating a significant performance bottleneck in real-world serving scenarios. To overcome this, we propose Nightjar, a novel learning-based algorithm for adaptive speculative inference that adjusts to request load by dynamically selecting the optimal speculative length for different batch sizes and even disabling speculative decoding when it provides no benefit. Experiments show that Nightjar achieves up to 14.8% higher throughput and 20.2% lower latency compared to standard speculative decoding, demonstrating robust efficiency for real-time serving.</p></details> | 6 pages, 11 figures |
| **[Optimal Program Synthesis via Abstract Interpretation](https://arxiv.org/abs/2602.14717v1)** | 2026-02-16 | <details><summary>Show</summary><p>We consider the problem of synthesizing programs with numerical constants that optimize a quantitative objective, such as accuracy, over a set of input-output examples. We propose a general framework for optimal synthesis of such programs in a given domain specific language (DSL), with provable optimality guarantees. Our framework enumerates programs in a general search graph, where nodes represent subsets of concrete programs. To improve scalability, it uses A* search in conjunction with a search heuristic based on abstract interpretation; intuitively, this heuristic establishes upper bounds on the value of subtrees in the search graph, enabling the synthesizer to identify and prune subtrees that are provably suboptimal. In addition, we propose a natural strategy for constructing abstract transformers for monotonic semantics, which is a common property for components in DSLs for data classification. Finally, we implement our approach in the context of two such existing DSLs, demonstrating that our algorithm is more scalable than existing optimal synthesizers.</p></details> |  |
| **[Evaluation of Dynamic Vector Bin Packing for Virtual Machine Placement](https://arxiv.org/abs/2602.14704v1)** | 2026-02-16 | <details><summary>Show</summary><p>Virtual machine placement is a crucial challenge in cloud computing for efficiently utilizing physical machine resources in data centers. Virtual machine placement can be formulated as a MinUsageTime Dynamic Vector Bin Packing (DVBP) problem, aiming to minimize the total usage time of the physical machines. This paper evaluates state-of-the-art MinUsageTime DVBP algorithms in non-clairvoyant, clairvoyant and learning-augmented online settings, where item durations (virtual machine lifetimes) are unknown, known and predicted, respectively. Besides the algorithms taken from the literature, we also develop several new algorithms or enhancements. Empirical experimentation is carried out with real-world datasets of Microsoft Azure. The insights from the experimental results are discussed to explore the structures of algorithms and promising design elements that work well in practice.</p></details> | <details><summary>Exten...</summary><p>Extended version of a paper that will appear in IEEE IPDPS 2026 conference</p></details> |
| **[LCLs Beyond Bounded Degrees](https://arxiv.org/abs/2602.02340v2)** | 2026-02-16 | <details><summary>Show</summary><p>The study of Locally Checkable Labelings (LCLs) has led to a remarkably precise characterization of the distributed time complexities that can occur on bounded-degree trees. A central feature of this complexity landscape is the existence of gap results, which rule out large ranges of intermediate complexities. While it was initially hoped that these gaps might extend to more general graph classes, this has turned out not to be the case. In this work, we investigate a different direction: we remain in the class of trees, but allow arbitrarily large degrees. We focus on the \emph{polynomial regime} ($Θ(n^{1/k} \mid k \in \mathbb{N})$) and show that whether polynomial gap results persist in the unbounded-degree setting depends on how LCLs are generalized beyond bounded degrees. There already exists a complex construction that shows that the polynomial gaps also vanish for LCLs on unbounded degree trees. Rather than stopping at this negative result, we give a much simpler set of problems that already contradicts the existence of any polynomial gaps. The insight obtained from this cleaner construction is that for gap results to exist, we cannot allow problems to distinguish infinitely many cases. This guides us to a natural class of problems for which polynomial gap results can still be recovered. We introduce \emph{Locally Finite Labelings} (LFLs), which formalize the intuition that \emph{every node must fall into one of finitely many local cases}. Our main result shows that this restriction is sufficient to restore the polynomial gaps: for any LFL $Π$ on trees with unbounded degrees, the deterministic LOCAL complexity of $Π$ is either - $Θ(n^{1/k})$ for some integer $k \geq 1$, or - $O(\log n)$. Moreover, which case applies, and the corresponding value of $k$, can be determined solely from the description of $Π$.</p></details> |  |
| **[Leader Election via Unique Sink Orientation](https://arxiv.org/abs/2511.19208v2)** | 2026-02-16 | <details><summary>Show</summary><p>A Locally Checkable Labeling (LCL) is a specification describing a set of labels that are valid with respect to a set of conditions that characterize a local part of a solution to a global problem. Conditions can only refer to nodes and labels within a constant radius neighborhood of each node. This work studies local labeling schemes whose global consistency implies solutions to two classical problems: leader election and spanning tree construction. For each problem, we present a local labeling scheme using one bit per edge or equivalently $O(Δ)$ bits per node (where $Δ$ is the maximum degree in the graph), with conditions checkable within the graph induced by the one neighborhood of each node. For leader election, we show that global satisfaction of the conditions implies the existence of a unique sink in the graph, which we define to be a leader, while in the spanning tree setting it implies that a specific subset of edges induces a spanning tree rooted at a given node. We show those implications for $K_4$-free dismantlable and chordal graphs in the former case and for dismantlable graphs in the latter, assuming a root is given. For chordal graphs, the labeling implying a unique sink additionally induces an acyclic orientation. This property is not generally locally verifiable with constant-size labels in arbitrary graphs. To the best of our knowledge, these are the first local labeling results tailored to ($K_4$-free) dismantlable graphs, potentially highlighting structural properties useful for designing LCLs for additional problems. Finally, we present a generic transformation that converts any local labeling scheme into a silent self-stabilizing algorithm by adding only one extra state, assuming a Gouda fair scheduler. This transformation may be of independent interest.</p></details> | <details><summary>19 pa...</summary><p>19 pages, 4 figures, under review</p></details> |
| **[Understanding GPU Resource Interference One Level Deeper](https://arxiv.org/abs/2501.16909v3)** | 2026-02-16 | <details><summary>Show</summary><p>GPUs are vastly underutilized, even when running resource-intensive AI applications, as GPU kernels within each job have diverse resource profiles that may saturate some parts of a device while often leaving other parts idle. Colocating applications is known to improve GPU utilization, but is not common practice as it becomes difficult to provide predictable performance due to workload interference. Providing predictable performance guarantees requires a deep understanding of how applications contend for shared GPU resources such as block schedulers, compute units, L1/L2 caches, and memory bandwidth. We study the key types of GPU resource interference and develop a methodology to quantify the sensitivity of a workload to each type. We discuss how this methodology can serve as the foundation for GPU schedulers that enforce strict performance guarantees and how application developers can design GPU kernels with colocation in mind to improve efficiency.</p></details> |  |
| **[Polar: An Algebraic Analyzer for (Probabilistic) Loops](https://arxiv.org/abs/2602.14573v1)** | 2026-02-16 | <details><summary>Show</summary><p>We present the Polar framework for fully automating the analysis of classical and probabilistic loops using algebraic reasoning. The central theme in Polar comes with handling algebraic recurrences that precisely capture the loop semantics. To this end, our work implements a variety of techniques to compute exact closed-forms of recurrences over higher-order moments of variables, infer invariants, and derive loop sensitivities with respect to unknown parameters. Polar can analyze probabilistic loops containing if-statements, polynomial arithmetic, and common probability distributions. By translating loop analysis into linear recurrence solving, Polar uses the derived closed-forms of recurrences to compute the strongest polynomial invariant or to infer parameter sensitivity. Polar is both sound and complete within well-defined programming model restrictions. Lifting any of these restrictions results in significant hardness limits of computation. To overcome computational burdens for the sake of efficiency, Polar also provides incomplete but sound techniques to compute moments of combinations of variables.</p></details> | <details><summary>Publi...</summary><p>Published in "Principles of Verification: Cycling the Probabilistic Landscape"</p></details> |
| **[Efficient Multi-round LLM Inference over Disaggregated Serving](https://arxiv.org/abs/2602.14516v1)** | 2026-02-16 | <details><summary>Show</summary><p>With the rapid evolution of Large Language Models (LLMs), multi-round workflows, such as autonomous agents and iterative retrieval, have become increasingly prevalent. However, this raises hurdles for serving LLMs under prefill-decode (PD) disaggregation, a widely adopted paradigm that separates the compute-bound prefill phase and memory-bound decode phase onto individual resources. Specifically, existing systems overlook the interleaved prefill-decode workload pattern in multi-round inference, leading to sub-optimal handling of the incremental prefill workloads and model deployment for the two phases. In this work, we present AMPD, a brand new disaggregated serving framework for multi-round LLM inference. The core of AMPD is to coordinate the prefill workloads based on real-time workloads by adaptively determining where to carry out these workloads and how they are scheduled, in order to maximize service level objective (SLO) attainment. In addition, we tailor a planning algorithm for our scenario, facilitating the deduction of optimal resource allocation and parallel strategies for the two phases. Empirical results demonstrate that AMPD substantially improves SLO attainment compared to state-of-the-art baselines.</p></details> |  |
| **[Evidential Trust-Aware Model Personalization in Decentralized Federated Learning for Wearable IoT](https://arxiv.org/abs/2512.19131v2)** | 2026-02-16 | <details><summary>Show</summary><p>Decentralized federated learning (DFL) enables collaborative model training across edge devices without centralized coordination, offering resilience against single points of failure. However, statistical heterogeneity arising from non-identically distributed local data creates a fundamental challenge: nodes must learn personalized models adapted to their local distributions while selectively collaborating with compatible peers. Existing approaches either enforce a single global model that fits no one well, or rely on heuristic peer selection mechanisms that cannot distinguish between peers with genuinely incompatible data distributions and those with valuable complementary knowledge. We present Murmura, a framework that leverages evidential deep learning to enable trust-aware model personalization in DFL. Our key insight is that epistemic uncertainty from Dirichlet-based evidential models directly indicates peer compatibility: high epistemic uncertainty when a peer's model evaluates local data reveals distributional mismatch, enabling nodes to exclude incompatible influence while maintaining personalized models through selective collaboration. Murmura introduces a trust-aware aggregation mechanism that computes peer compatibility scores through cross-evaluation on local validation samples and personalizes model aggregation based on evidential trust with adaptive thresholds. Evaluation on three wearable IoT datasets (UCI HAR, PAMAP2, PPG-DaLiA) demonstrates that Murmura reduces performance degradation from IID to non-IID conditions compared to baseline (0.9% vs. 19.3%), achieves 7.4$\times$ faster convergence, and maintains stable accuracy across hyperparameter choices. These results establish evidential uncertainty as a principled foundation for compatibility-aware personalization in decentralized heterogeneous environments.</p></details> | <details><summary>v2. A...</summary><p>v2. Addressed minor reviewer concerns</p></details> |
| **[Floe: Federated Specialization for Real-Time LLM-SLM Inference](https://arxiv.org/abs/2602.14302v1)** | 2026-02-15 | <details><summary>Show</summary><p>Deploying large language models (LLMs) in real-time systems remains challenging due to their substantial computational demands and privacy concerns. We propose Floe, a hybrid federated learning framework designed for latency-sensitive, resource-constrained environments. Floe combines a cloud-based black-box LLM with lightweight small language models (SLMs) on edge devices to enable low-latency, privacy-preserving inference. Personal data and fine-tuning remain on-device, while the cloud LLM contributes general knowledge without exposing proprietary weights. A heterogeneity-aware LoRA adaptation strategy enables efficient edge deployment across diverse hardware, and a logit-level fusion mechanism enables real-time coordination between edge and cloud models. Extensive experiments demonstrate that Floe enhances user privacy and personalization. Moreover, it significantly improves model performance and reduces inference latency on edge devices under real-time constraints compared with baseline approaches.</p></details> | <details><summary>Accep...</summary><p>Accepted by IEEE Transactions on Parallel and Distributed Systems</p></details> |
| **[Parallel Sparse and Data-Sparse Factorization-based Linear Solvers](https://arxiv.org/abs/2602.14289v1)** | 2026-02-15 | <details><summary>Show</summary><p>Efficient solutions of large-scale, ill-conditioned and indefinite algebraic equations are ubiquitously needed in numerous computational fields, including multiphysics simulations, machine learning, and data science. Because of their robustness and accuracy, direct solvers are crucial components in building a scalable solver toolchain. In this article, we will review recent advances of sparse direct solvers along two axes: 1) reducing communication and latency costs in both task- and data-parallel settings, and 2) reducing computational complexity via low-rank and other compression techniques such as hierarchical matrix algebra. In addition to algorithmic principles, we also illustrate the key parallelization challenges and best practices to deliver high speed and reliability on modern heterogeneous parallel machines.</p></details> |  |
| **[SlimEdge: Performance and Device Aware Distributed DNN Deployment on Resource-Constrained Edge Hardware](https://arxiv.org/abs/2512.22136v2)** | 2026-02-15 | <details><summary>Show</summary><p>Distributed deep neural networks (DNNs) have become central to modern computer vision, yet their deployment on resource-constrained edge devices remains hindered by substantial parameter counts, computational demands, and the probability of device failure. Here, we present an approach to the efficient deployment of distributed DNNs that jointly respect hardware limitations, preserve task performance, and remain robust to partial system failures. Our method integrates structured model pruning with a multi-objective optimization framework to tailor network capacity for heterogeneous device constraints, while explicitly accounting for device availability and failure probability during deployment. We demonstrate this framework using Multi-View Convolutional Neural Networks (MVCNN), a state-of-the-art architecture for 3D object recognition, by quantifying the contribution of individual views to classification accuracy and allocating pruning budgets accordingly. Experimental results show that the resulting models satisfy user-specified bounds on accuracy and memory footprint, even under multiple simultaneous device failures. The inference time is reduced by factors up to 4.7x across diverse simulated device configurations. These findings suggest that performance-aware, view-adaptive, and failure-resilient compression provides a viable pathway for deploying complex vision models in distributed edge environments.</p></details> |  |
| **[Dual-Signal Adaptive KV-Cache Optimization for Long-Form Video Understanding in Vision-Language Models](https://arxiv.org/abs/2602.14236v1)** | 2026-02-15 | <details><summary>Show</summary><p>Vision-Language Models (VLMs) face a critical memory bottleneck when processing long-form video content due to the linear growth of the Key-Value (KV) cache with sequence length. Existing solutions predominantly employ reactive eviction strategies that compute full attention matrices before discarding tokens, resulting in substantial computational waste. We propose Sali-Cache, a novel a priori optimization framework that implements dual-signal adaptive caching through proactive memory management. By integrating a temporal filter based on optical flow analysis for detecting inter-frame redundancy and a spatial filter leveraging saliency detection for identifying visually significant regions, Sali-Cache intelligently manages memory allocation before entering computationally expensive attention operations. Experimental evaluation on the LLaVA 1.6 architecture demonstrates that our method achieves a 2.20x compression ratio in effective memory usage while maintaining 100% accuracy across BLEU, ROUGE-L, and Exact Match metrics. Furthermore, under identical memory budget constraints, Sali-Cache preserves context-rich features over extended temporal durations without degrading model performance, enabling efficient processing of long-form video content on consumer-grade hardware.</p></details> |  |
| **[Optimizing Task Scheduling in Fog Computing with Deadline Awareness](https://arxiv.org/abs/2509.07378v5)** | 2026-02-15 | <details><summary>Show</summary><p>The rise of Internet of Things (IoT) devices has led to the development of numerous time-sensitive applications that require quick responses and low latency. Fog computing has emerged as a solution for processing these IoT applications, but it faces challenges such as resource allocation and job scheduling. Therefore, it is crucial to determine how to assign and schedule tasks on Fog nodes. This work aims to schedule tasks in IoT while minimizing the total energy consumption of nodes and enhancing the Quality of Service (QoS) requirements of IoT tasks, taking into account task deadlines. This paper classifies Fog nodes into two categories based on their traffic level: low and high. It schedules short-deadline tasks on low-traffic nodes using an Improved Golden Eagle Optimization (IGEO) algorithm, an enhancement that utilizes genetic operators for discretization. Long-deadline tasks are processed on high-traffic nodes using reinforcement learning (RL). This combined approach is called the Reinforcement Improved Golden Eagle Optimization (RIGEO) algorithm. Experimental results demonstrate that RIGEO achieves up to a 29% reduction in energy consumption, up to an 86% improvement in response time, and up to a 19% reduction in deadline violations compared to state-of-the-art algorithms.</p></details> |  |
| **[ML-ECS: A Collaborative Multimodal Learning Framework for Edge-Cloud Synergies](https://arxiv.org/abs/2602.14107v1)** | 2026-02-15 | <details><summary>Show</summary><p>Edge-cloud synergies provide a promising paradigm for privacy-preserving deployment of foundation models, where lightweight on-device models adapt to domain-specific data and cloud-hosted models coordinate knowledge sharing. However, in real-world edge environments, collaborative multimodal learning is challenged by modality heterogeneity (different modality combinations across domains) and model-structure heterogeneity (different modality-specific encoders/fusion modules. To address these issues, we propose ML-ECS, a collaborative multimodal learning framework that enables joint training between a server-based model and heterogeneous edge models. This framework consists of four components: (1) cross-modal contrastive learning (CCL) to align modality representations in a shared latent space, (2) adaptive multimodal tuning (AMT) to preserve domain-specific knowledge from local datasets, (3) modality-aware model aggregation (MMA) to robustly aggregate while mitigating noise caused by missing modalities, and (4) SLM-enhanced CCL (SE-CCL) to facilitate bidirectional knowledge transfer between cloud and edge. Experimental results on various multimodal tasks show that \pname consistently outperform state-of-the-art baselines under varying modality availability, achieving improvements of 5.44% to 12.08% in Rouge-LSum and improving both client- and server-side performance. In addition, by communicating only low-rank LoRA parameters and fused representations, ML-ECS achieves high communication efficiency, requiring only 0.65% of the total parameter volume.</p></details> |  |

## performance
| **Title** | **Date** | **Abstract** | **Comment** |
| --- | --- | --- | --- |
| **[Stable-MoE: Lyapunov-based Token Routing for Distributed Mixture-of-Experts Training over Edge Networks](https://arxiv.org/abs/2512.06784v2)** | 2026-02-16 | <details><summary>Show</summary><p>The sparse activation mechanism of mixture of experts (MoE) model empowers edge intelligence with enhanced training efficiency and reduced computational resource consumption. However, traditional token routing in distributed MoE training faces significant challenges in resource-constrained edge networks characterized by heterogeneous computing capabilities and stochastic token arrivals, which inevitably suffer from workload backlog, resource inefficiency, and performance degradation. To address this issue, we propose a novel Lyapunov-based token routing framework for distributed MoE training over resource-heterogeneous edge networks, termed Stable-MoE. Specifically, we formulate a stochastic optimization problem to maximize both system throughput and gating consistency via optimizing the token routing strategy and computational resource allocation, while ensuring long-term stability of both token and energy queues at the edge devices. Using the Lyapunov optimization, we transform the intractable long-term optimization problem into tractable per-slot subproblems by enabling online decision-making of token routing and computation frequency utilization without the knowledge of future system states. Experimental results on the SVHN and CIFAR-100 datasets demonstrate that Stable-MoE outperforms the baselines with at least 40% and 5% gains in system throughput and test accuracy, respectively.</p></details> |  |
| **[Transforming Lock-free Linked Lists into Distributed Lock-free Linked Lists](https://arxiv.org/abs/2510.06387v2)** | 2026-02-16 | <details><summary>Show</summary><p>Modern databases use dynamic search structures that store an enormous amount of data, and often serve them using multi-threaded algorithms to support the ever-increasing throughput needs. When this throughput need exceeds the capacity of the machine hosting the structure, one either needs to replace the underlying hardware (an option that is typically not viable and introduces a long down time) or make the data structure distributed. Static partitioning of the data structure for distribution is not desirable, as it is prone to uneven load distribution over time, and having to change the partitioning scheme later will require downtime. The goal of this paper is to extend a concurrent data structure to distributed data structures that provide dynamic load balancing while preserving important properties such as lock freedom. With this intuition, first, we introduce the notion of conditional lock-freedom that extends the notion of lock-free computation with reasonable assumptions about communication between processes. Then, we present DiLi, a conditional lock-free, linearizable, and distributable linked list that can be asynchronously and dynamically (1) partitioned into multiple sublists and (2) load balanced by distributing sublists across multiple machines. DiLi contains primitives for these that also maintain the lock-free property of the underlying search structure that supports find, remove, and insert of a key as the client operations. We show that DiLi bridges the gap between concurrent data structures and distributed data structures. Specifically, DiLi provides comparable (and better in write-intensive workloads) performance to skip lists (which are typically the fastest data structures for search in a concurrent environment). Plus, it provides horizontal scaling by permitting dynamic load balancing on a distributed environment.</p></details> |  |
| **[Low-Latency Federated Fine-Tuning for Large Language Models Over Wireless Networks](https://arxiv.org/abs/2602.01024v2)** | 2026-02-16 | <details><summary>Show</summary><p>Recently, federated large language models (LLMs) have drawn significant attention thanks to coupled capabilities of LLMs and federated learning (FL) that address privacy concerns in collaborative fine-tuning. However, due to large-scale parameters of LLMs, existing federated LLM fine-tuning frameworks incur significant challenges in resource-constrained clients characterized by heterogeneous computing capabilities and random wireless channels. To address this issue, we propose a joint client-specific pruning and bandwidth allocation (JCPBA) framework for federated LLMs to improve the fine-tuning efficiency over the wireless networks. Specifically, we formulate a fine-tuning latency minimization problem by jointly optimizing pruning rates and bandwidth allocations. Furthermore, we solve this optimization problem using a block coordinate descent method. Extensive experiments on the datasets of Yahoo Answers and GSM8K demonstrate that the proposed framework significantly reduces wall-clock fine-tuning time compared with state-of-the-art baselines and gains equal or lower test loss at the cost of lower computation and communication overhead.</p></details> |  |
| **[EdgeFaaS: A Function-based Framework for Edge Computing](https://arxiv.org/abs/2210.01410v2)** | 2026-02-16 | <details><summary>Show</summary><p>The rapid growth of data generated from Internet of Things (IoTs) such as smart phones and smart home devices presents new challenges to cloud computing in transferring, storing, and processing the data. With increasingly more powerful edge devices, edge computing, on the other hand, has the potential to better responsiveness, privacy, and cost efficiency. However, resources across the cloud and edge are highly distributed and highly diverse. To address these challenges, this paper proposes EdgeFaaS, a Function-as-a-Service (FaaS) based computing framework that supports the flexible, convenient, and optimized use of distributed and heterogeneous resources across IoT, edge, and cloud systems. EdgeFaaS allows cluster resources and individual devices to be managed under the same framework and provide computational and storage resources for functions. It provides virtual function and virtual storage interfaces for consistent function management and storage management across heterogeneous compute and storage resources. It automatically optimizes the scheduling of functions and placement of data according to their performance and privacy requirements. EdgeFaaS is evaluated based on two edge workflows: video analytics workflow and federated learning workflow, both of which are representative edge applications and involve large amounts of input data generated from edge devices.</p></details> | <details><summary>This ...</summary><p>This paper has been withdrawn by the authors because it was submitted without the final consent of all co-authors</p></details> |
| **[Atomix: Timely, Transactional Tool Use for Reliable Agentic Workflows](https://arxiv.org/abs/2602.14849v1)** | 2026-02-16 | <details><summary>Show</summary><p>LLM agents increasingly act on external systems, yet tool effects are immediate. Under failures, speculation, or contention, losing branches can leak unintended side effects with no safe rollback. We introduce Atomix, a runtime that provides progress-aware transactional semantics for agent tool calls. Atomix tags each call with an epoch, tracks per-resource frontiers, and commits only when progress predicates indicate safety; bufferable effects can be delayed, while externalized effects are tracked and compensated on abort. Across real workloads with fault injection, transactional retry improves task success, while frontier-gated commit strengthens isolation under speculation and contention.</p></details> |  |
| **[Nightjar: Dynamic Adaptive Speculative Decoding for Large Language Models Serving](https://arxiv.org/abs/2512.22420v2)** | 2026-02-16 | <details><summary>Show</summary><p>Speculative decoding (SD) accelerates LLM inference by verifying draft tokens in parallel. However, this method presents a critical trade-off: it improves throughput in low-load, memory-bound systems but degrades performance in high-load, compute-bound environments due to verification overhead. Current SD implementations use a fixed speculative length, failing to adapt to dynamic request rates and creating a significant performance bottleneck in real-world serving scenarios. To overcome this, we propose Nightjar, a novel learning-based algorithm for adaptive speculative inference that adjusts to request load by dynamically selecting the optimal speculative length for different batch sizes and even disabling speculative decoding when it provides no benefit. Experiments show that Nightjar achieves up to 14.8% higher throughput and 20.2% lower latency compared to standard speculative decoding, demonstrating robust efficiency for real-time serving.</p></details> | 6 pages, 11 figures |
| **[Optimal Program Synthesis via Abstract Interpretation](https://arxiv.org/abs/2602.14717v1)** | 2026-02-16 | <details><summary>Show</summary><p>We consider the problem of synthesizing programs with numerical constants that optimize a quantitative objective, such as accuracy, over a set of input-output examples. We propose a general framework for optimal synthesis of such programs in a given domain specific language (DSL), with provable optimality guarantees. Our framework enumerates programs in a general search graph, where nodes represent subsets of concrete programs. To improve scalability, it uses A* search in conjunction with a search heuristic based on abstract interpretation; intuitively, this heuristic establishes upper bounds on the value of subtrees in the search graph, enabling the synthesizer to identify and prune subtrees that are provably suboptimal. In addition, we propose a natural strategy for constructing abstract transformers for monotonic semantics, which is a common property for components in DSLs for data classification. Finally, we implement our approach in the context of two such existing DSLs, demonstrating that our algorithm is more scalable than existing optimal synthesizers.</p></details> |  |
| **[Evaluation of Dynamic Vector Bin Packing for Virtual Machine Placement](https://arxiv.org/abs/2602.14704v1)** | 2026-02-16 | <details><summary>Show</summary><p>Virtual machine placement is a crucial challenge in cloud computing for efficiently utilizing physical machine resources in data centers. Virtual machine placement can be formulated as a MinUsageTime Dynamic Vector Bin Packing (DVBP) problem, aiming to minimize the total usage time of the physical machines. This paper evaluates state-of-the-art MinUsageTime DVBP algorithms in non-clairvoyant, clairvoyant and learning-augmented online settings, where item durations (virtual machine lifetimes) are unknown, known and predicted, respectively. Besides the algorithms taken from the literature, we also develop several new algorithms or enhancements. Empirical experimentation is carried out with real-world datasets of Microsoft Azure. The insights from the experimental results are discussed to explore the structures of algorithms and promising design elements that work well in practice.</p></details> | <details><summary>Exten...</summary><p>Extended version of a paper that will appear in IEEE IPDPS 2026 conference</p></details> |
| **[LCLs Beyond Bounded Degrees](https://arxiv.org/abs/2602.02340v2)** | 2026-02-16 | <details><summary>Show</summary><p>The study of Locally Checkable Labelings (LCLs) has led to a remarkably precise characterization of the distributed time complexities that can occur on bounded-degree trees. A central feature of this complexity landscape is the existence of gap results, which rule out large ranges of intermediate complexities. While it was initially hoped that these gaps might extend to more general graph classes, this has turned out not to be the case. In this work, we investigate a different direction: we remain in the class of trees, but allow arbitrarily large degrees. We focus on the \emph{polynomial regime} ($Θ(n^{1/k} \mid k \in \mathbb{N})$) and show that whether polynomial gap results persist in the unbounded-degree setting depends on how LCLs are generalized beyond bounded degrees. There already exists a complex construction that shows that the polynomial gaps also vanish for LCLs on unbounded degree trees. Rather than stopping at this negative result, we give a much simpler set of problems that already contradicts the existence of any polynomial gaps. The insight obtained from this cleaner construction is that for gap results to exist, we cannot allow problems to distinguish infinitely many cases. This guides us to a natural class of problems for which polynomial gap results can still be recovered. We introduce \emph{Locally Finite Labelings} (LFLs), which formalize the intuition that \emph{every node must fall into one of finitely many local cases}. Our main result shows that this restriction is sufficient to restore the polynomial gaps: for any LFL $Π$ on trees with unbounded degrees, the deterministic LOCAL complexity of $Π$ is either - $Θ(n^{1/k})$ for some integer $k \geq 1$, or - $O(\log n)$. Moreover, which case applies, and the corresponding value of $k$, can be determined solely from the description of $Π$.</p></details> |  |
| **[Leader Election via Unique Sink Orientation](https://arxiv.org/abs/2511.19208v2)** | 2026-02-16 | <details><summary>Show</summary><p>A Locally Checkable Labeling (LCL) is a specification describing a set of labels that are valid with respect to a set of conditions that characterize a local part of a solution to a global problem. Conditions can only refer to nodes and labels within a constant radius neighborhood of each node. This work studies local labeling schemes whose global consistency implies solutions to two classical problems: leader election and spanning tree construction. For each problem, we present a local labeling scheme using one bit per edge or equivalently $O(Δ)$ bits per node (where $Δ$ is the maximum degree in the graph), with conditions checkable within the graph induced by the one neighborhood of each node. For leader election, we show that global satisfaction of the conditions implies the existence of a unique sink in the graph, which we define to be a leader, while in the spanning tree setting it implies that a specific subset of edges induces a spanning tree rooted at a given node. We show those implications for $K_4$-free dismantlable and chordal graphs in the former case and for dismantlable graphs in the latter, assuming a root is given. For chordal graphs, the labeling implying a unique sink additionally induces an acyclic orientation. This property is not generally locally verifiable with constant-size labels in arbitrary graphs. To the best of our knowledge, these are the first local labeling results tailored to ($K_4$-free) dismantlable graphs, potentially highlighting structural properties useful for designing LCLs for additional problems. Finally, we present a generic transformation that converts any local labeling scheme into a silent self-stabilizing algorithm by adding only one extra state, assuming a Gouda fair scheduler. This transformation may be of independent interest.</p></details> | <details><summary>19 pa...</summary><p>19 pages, 4 figures, under review</p></details> |
| **[Understanding GPU Resource Interference One Level Deeper](https://arxiv.org/abs/2501.16909v3)** | 2026-02-16 | <details><summary>Show</summary><p>GPUs are vastly underutilized, even when running resource-intensive AI applications, as GPU kernels within each job have diverse resource profiles that may saturate some parts of a device while often leaving other parts idle. Colocating applications is known to improve GPU utilization, but is not common practice as it becomes difficult to provide predictable performance due to workload interference. Providing predictable performance guarantees requires a deep understanding of how applications contend for shared GPU resources such as block schedulers, compute units, L1/L2 caches, and memory bandwidth. We study the key types of GPU resource interference and develop a methodology to quantify the sensitivity of a workload to each type. We discuss how this methodology can serve as the foundation for GPU schedulers that enforce strict performance guarantees and how application developers can design GPU kernels with colocation in mind to improve efficiency.</p></details> |  |
| **[Polar: An Algebraic Analyzer for (Probabilistic) Loops](https://arxiv.org/abs/2602.14573v1)** | 2026-02-16 | <details><summary>Show</summary><p>We present the Polar framework for fully automating the analysis of classical and probabilistic loops using algebraic reasoning. The central theme in Polar comes with handling algebraic recurrences that precisely capture the loop semantics. To this end, our work implements a variety of techniques to compute exact closed-forms of recurrences over higher-order moments of variables, infer invariants, and derive loop sensitivities with respect to unknown parameters. Polar can analyze probabilistic loops containing if-statements, polynomial arithmetic, and common probability distributions. By translating loop analysis into linear recurrence solving, Polar uses the derived closed-forms of recurrences to compute the strongest polynomial invariant or to infer parameter sensitivity. Polar is both sound and complete within well-defined programming model restrictions. Lifting any of these restrictions results in significant hardness limits of computation. To overcome computational burdens for the sake of efficiency, Polar also provides incomplete but sound techniques to compute moments of combinations of variables.</p></details> | <details><summary>Publi...</summary><p>Published in "Principles of Verification: Cycling the Probabilistic Landscape"</p></details> |
| **[Efficient Multi-round LLM Inference over Disaggregated Serving](https://arxiv.org/abs/2602.14516v1)** | 2026-02-16 | <details><summary>Show</summary><p>With the rapid evolution of Large Language Models (LLMs), multi-round workflows, such as autonomous agents and iterative retrieval, have become increasingly prevalent. However, this raises hurdles for serving LLMs under prefill-decode (PD) disaggregation, a widely adopted paradigm that separates the compute-bound prefill phase and memory-bound decode phase onto individual resources. Specifically, existing systems overlook the interleaved prefill-decode workload pattern in multi-round inference, leading to sub-optimal handling of the incremental prefill workloads and model deployment for the two phases. In this work, we present AMPD, a brand new disaggregated serving framework for multi-round LLM inference. The core of AMPD is to coordinate the prefill workloads based on real-time workloads by adaptively determining where to carry out these workloads and how they are scheduled, in order to maximize service level objective (SLO) attainment. In addition, we tailor a planning algorithm for our scenario, facilitating the deduction of optimal resource allocation and parallel strategies for the two phases. Empirical results demonstrate that AMPD substantially improves SLO attainment compared to state-of-the-art baselines.</p></details> |  |
| **[Evidential Trust-Aware Model Personalization in Decentralized Federated Learning for Wearable IoT](https://arxiv.org/abs/2512.19131v2)** | 2026-02-16 | <details><summary>Show</summary><p>Decentralized federated learning (DFL) enables collaborative model training across edge devices without centralized coordination, offering resilience against single points of failure. However, statistical heterogeneity arising from non-identically distributed local data creates a fundamental challenge: nodes must learn personalized models adapted to their local distributions while selectively collaborating with compatible peers. Existing approaches either enforce a single global model that fits no one well, or rely on heuristic peer selection mechanisms that cannot distinguish between peers with genuinely incompatible data distributions and those with valuable complementary knowledge. We present Murmura, a framework that leverages evidential deep learning to enable trust-aware model personalization in DFL. Our key insight is that epistemic uncertainty from Dirichlet-based evidential models directly indicates peer compatibility: high epistemic uncertainty when a peer's model evaluates local data reveals distributional mismatch, enabling nodes to exclude incompatible influence while maintaining personalized models through selective collaboration. Murmura introduces a trust-aware aggregation mechanism that computes peer compatibility scores through cross-evaluation on local validation samples and personalizes model aggregation based on evidential trust with adaptive thresholds. Evaluation on three wearable IoT datasets (UCI HAR, PAMAP2, PPG-DaLiA) demonstrates that Murmura reduces performance degradation from IID to non-IID conditions compared to baseline (0.9% vs. 19.3%), achieves 7.4$\times$ faster convergence, and maintains stable accuracy across hyperparameter choices. These results establish evidential uncertainty as a principled foundation for compatibility-aware personalization in decentralized heterogeneous environments.</p></details> | <details><summary>v2. A...</summary><p>v2. Addressed minor reviewer concerns</p></details> |
| **[Floe: Federated Specialization for Real-Time LLM-SLM Inference](https://arxiv.org/abs/2602.14302v1)** | 2026-02-15 | <details><summary>Show</summary><p>Deploying large language models (LLMs) in real-time systems remains challenging due to their substantial computational demands and privacy concerns. We propose Floe, a hybrid federated learning framework designed for latency-sensitive, resource-constrained environments. Floe combines a cloud-based black-box LLM with lightweight small language models (SLMs) on edge devices to enable low-latency, privacy-preserving inference. Personal data and fine-tuning remain on-device, while the cloud LLM contributes general knowledge without exposing proprietary weights. A heterogeneity-aware LoRA adaptation strategy enables efficient edge deployment across diverse hardware, and a logit-level fusion mechanism enables real-time coordination between edge and cloud models. Extensive experiments demonstrate that Floe enhances user privacy and personalization. Moreover, it significantly improves model performance and reduces inference latency on edge devices under real-time constraints compared with baseline approaches.</p></details> | <details><summary>Accep...</summary><p>Accepted by IEEE Transactions on Parallel and Distributed Systems</p></details> |
| **[Parallel Sparse and Data-Sparse Factorization-based Linear Solvers](https://arxiv.org/abs/2602.14289v1)** | 2026-02-15 | <details><summary>Show</summary><p>Efficient solutions of large-scale, ill-conditioned and indefinite algebraic equations are ubiquitously needed in numerous computational fields, including multiphysics simulations, machine learning, and data science. Because of their robustness and accuracy, direct solvers are crucial components in building a scalable solver toolchain. In this article, we will review recent advances of sparse direct solvers along two axes: 1) reducing communication and latency costs in both task- and data-parallel settings, and 2) reducing computational complexity via low-rank and other compression techniques such as hierarchical matrix algebra. In addition to algorithmic principles, we also illustrate the key parallelization challenges and best practices to deliver high speed and reliability on modern heterogeneous parallel machines.</p></details> |  |
| **[SlimEdge: Performance and Device Aware Distributed DNN Deployment on Resource-Constrained Edge Hardware](https://arxiv.org/abs/2512.22136v2)** | 2026-02-15 | <details><summary>Show</summary><p>Distributed deep neural networks (DNNs) have become central to modern computer vision, yet their deployment on resource-constrained edge devices remains hindered by substantial parameter counts, computational demands, and the probability of device failure. Here, we present an approach to the efficient deployment of distributed DNNs that jointly respect hardware limitations, preserve task performance, and remain robust to partial system failures. Our method integrates structured model pruning with a multi-objective optimization framework to tailor network capacity for heterogeneous device constraints, while explicitly accounting for device availability and failure probability during deployment. We demonstrate this framework using Multi-View Convolutional Neural Networks (MVCNN), a state-of-the-art architecture for 3D object recognition, by quantifying the contribution of individual views to classification accuracy and allocating pruning budgets accordingly. Experimental results show that the resulting models satisfy user-specified bounds on accuracy and memory footprint, even under multiple simultaneous device failures. The inference time is reduced by factors up to 4.7x across diverse simulated device configurations. These findings suggest that performance-aware, view-adaptive, and failure-resilient compression provides a viable pathway for deploying complex vision models in distributed edge environments.</p></details> |  |
| **[Dual-Signal Adaptive KV-Cache Optimization for Long-Form Video Understanding in Vision-Language Models](https://arxiv.org/abs/2602.14236v1)** | 2026-02-15 | <details><summary>Show</summary><p>Vision-Language Models (VLMs) face a critical memory bottleneck when processing long-form video content due to the linear growth of the Key-Value (KV) cache with sequence length. Existing solutions predominantly employ reactive eviction strategies that compute full attention matrices before discarding tokens, resulting in substantial computational waste. We propose Sali-Cache, a novel a priori optimization framework that implements dual-signal adaptive caching through proactive memory management. By integrating a temporal filter based on optical flow analysis for detecting inter-frame redundancy and a spatial filter leveraging saliency detection for identifying visually significant regions, Sali-Cache intelligently manages memory allocation before entering computationally expensive attention operations. Experimental evaluation on the LLaVA 1.6 architecture demonstrates that our method achieves a 2.20x compression ratio in effective memory usage while maintaining 100% accuracy across BLEU, ROUGE-L, and Exact Match metrics. Furthermore, under identical memory budget constraints, Sali-Cache preserves context-rich features over extended temporal durations without degrading model performance, enabling efficient processing of long-form video content on consumer-grade hardware.</p></details> |  |
| **[Optimizing Task Scheduling in Fog Computing with Deadline Awareness](https://arxiv.org/abs/2509.07378v5)** | 2026-02-15 | <details><summary>Show</summary><p>The rise of Internet of Things (IoT) devices has led to the development of numerous time-sensitive applications that require quick responses and low latency. Fog computing has emerged as a solution for processing these IoT applications, but it faces challenges such as resource allocation and job scheduling. Therefore, it is crucial to determine how to assign and schedule tasks on Fog nodes. This work aims to schedule tasks in IoT while minimizing the total energy consumption of nodes and enhancing the Quality of Service (QoS) requirements of IoT tasks, taking into account task deadlines. This paper classifies Fog nodes into two categories based on their traffic level: low and high. It schedules short-deadline tasks on low-traffic nodes using an Improved Golden Eagle Optimization (IGEO) algorithm, an enhancement that utilizes genetic operators for discretization. Long-deadline tasks are processed on high-traffic nodes using reinforcement learning (RL). This combined approach is called the Reinforcement Improved Golden Eagle Optimization (RIGEO) algorithm. Experimental results demonstrate that RIGEO achieves up to a 29% reduction in energy consumption, up to an 86% improvement in response time, and up to a 19% reduction in deadline violations compared to state-of-the-art algorithms.</p></details> |  |
| **[ML-ECS: A Collaborative Multimodal Learning Framework for Edge-Cloud Synergies](https://arxiv.org/abs/2602.14107v1)** | 2026-02-15 | <details><summary>Show</summary><p>Edge-cloud synergies provide a promising paradigm for privacy-preserving deployment of foundation models, where lightweight on-device models adapt to domain-specific data and cloud-hosted models coordinate knowledge sharing. However, in real-world edge environments, collaborative multimodal learning is challenged by modality heterogeneity (different modality combinations across domains) and model-structure heterogeneity (different modality-specific encoders/fusion modules. To address these issues, we propose ML-ECS, a collaborative multimodal learning framework that enables joint training between a server-based model and heterogeneous edge models. This framework consists of four components: (1) cross-modal contrastive learning (CCL) to align modality representations in a shared latent space, (2) adaptive multimodal tuning (AMT) to preserve domain-specific knowledge from local datasets, (3) modality-aware model aggregation (MMA) to robustly aggregate while mitigating noise caused by missing modalities, and (4) SLM-enhanced CCL (SE-CCL) to facilitate bidirectional knowledge transfer between cloud and edge. Experimental results on various multimodal tasks show that \pname consistently outperform state-of-the-art baselines under varying modality availability, achieving improvements of 5.44% to 12.08% in Rouge-LSum and improving both client- and server-side performance. In addition, by communicating only low-rank LoRA parameters and fused representations, ML-ECS achieves high communication efficiency, requiring only 0.65% of the total parameter volume.</p></details> |  |

