# Daily Papers
The project automatically fetches the latest papers from arXiv based on keywords.

The subheadings in the README file represent the search keywords.

Only the most recent articles for each keyword are retained, up to a maximum of 100 papers.

You can click the 'Watch' button to receive daily email notifications.

Last update: 2025-12-16

## reinforcement learning
| **Title** | **Date** | **Abstract** | **Comment** |
| --- | --- | --- | --- |
| **[Hypergraph based Multi-Party Payment Channel](https://arxiv.org/abs/2512.11775v1)** | 2025-12-12 | <details><summary>Show</summary><p>Public blockchains inherently offer low throughput and high latency, motivating off-chain scalability solutions such as Payment Channel Networks (PCNs). However, existing PCNs suffer from liquidity fragmentation-funds locked in one channel cannot be reused elsewhere-and channel depletion, both of which limit routing efficiency and reduce transaction success rates. Multi-party channel (MPC) constructions mitigate these issues, but they typically rely on leaders or coordinators, creating single points of failure and providing only limited flexibility for inter-channel payments. We introduce Hypergraph-based Multi-Party Payment Channels (H-MPCs), a new off-chain construction that replaces bilateral channels with collectively funded hyperedges. These hyperedges enable fully concurrent, leaderless intra- and inter-hyperedge payments through verifiable, proposer-ordered DAG updates, offering significantly greater flexibility and concurrency than prior designs. Our implementation on a 150-node network demonstrates a transaction success rate of approximately 94% without HTLC expiry or routing failures, highlighting the robustness of H-MPCs.</p></details> |  |
| **[The Relative Monadic Metalanguage](https://arxiv.org/abs/2512.11762v1)** | 2025-12-12 | <details><summary>Show</summary><p>Relative monads provide a controlled view of computation. We generalise the monadic metalanguage to a relative setting and give a complete semantics with strong relative monads. Adopting this perspective, we generalise two existing program calculi from the literature. We provide a linear-non-linear language for graded monads, LNL-RMM, along with a semantic proof that it is a conservative extension of the graded monadic metalanguage. Additionally, we provide a complete semantics for the arrow calculus, showing it is a restricted relative monadic metalanguage. This motivates the introduction of ARMM, a computational lambda calculus-style language for arrows that conservatively extends the arrow calculus.</p></details> | <details><summary>41 pa...</summary><p>41 pages. Published in Proceedings of the ACM on Programming Languages (POPL 2026)</p></details> |
| **[REASONING COMPILER: LLM-Guided Optimizations for Efficient Model Serving](https://arxiv.org/abs/2506.01374v4)** | 2025-12-12 | <details><summary>Show</summary><p>While model serving has unlocked unprecedented capabilities, the high cost of serving large-scale models continues to be a significant barrier to widespread accessibility and rapid innovation. Compiler optimizations have long driven substantial performance improvements, but existing compilers struggle with neural workloads due to the exponentially large and highly interdependent space of possible transformations. Although existing stochastic search techniques can be effective, they are often sample-inefficient and fail to leverage the structural context underlying compilation decisions. We set out to investigate the research question of whether reasoning with large language models (LLMs), without any retraining, can leverage the context-aware decision space of compiler optimizations to significantly improve sample efficiency. To that end, we introduce a novel compilation framework (dubbed Reasoning Compiler) that formulates optimization as a sequential, context-aware decision process guided by a large language model and structured Monte Carlo tree search (MCTS). The LLM acts as a proposal mechanism, suggesting hardware-informed transformations that reflect the current program state and accumulated performance feedback. MCTS incorporates the LLM-generated proposals to balance exploration and exploitation, facilitating structured, context-sensitive traversal of the expansive compiler optimization space. By achieving substantial speedups with markedly fewer samples than leading neural compilers, our approach demonstrates the potential of LLM-guided reasoning to transform the landscape of compiler optimization.</p></details> | NeurIPS 2025 |
| **[TD-Orch: Scalable Load-Balancing for Distributed Systems with Applications to Graph Processing](https://arxiv.org/abs/2511.11843v2)** | 2025-12-12 | <details><summary>Show</summary><p>In this paper, we introduce a task-data orchestration abstraction that supports a range of distributed applications, including graph processing and key-value stores. Given a batch of lambda tasks each requesting one or more data items, where both tasks and data are distributed across multiple machines, each task must be co-located with its target data (by moving tasks and/or data) and then executed. We present TD-Orch, an efficient and scalable orchestration framework featuring a simple application developer interface. TD-Orch employs a distributed push-pull technique, leveraging the bidirectional flow of both tasks and data to achieve scalable load balance across machines even under highly skewed data requests (data hot spots), with minimal communication overhead. Experimental results show that TD-Orch achieves up to 2.8x speedup over existing distributed scheduling baselines. Building on TD-Orch, we present TDO-GP, a distributed graph processing system for general graph problems, demonstrating the effectiveness of the underlying framework. We design three families of implementation techniques to fully leverage the execution flow provided by TD-Orch. Experimental results show that TDO-GP achieves an average speedup of 4.1x over the best prior open-source distributed graph systems for general graph processing.</p></details> |  |
| **[ECCO: Leveraging Cross-Camera Correlations for Efficient Live Video Continuous Learning](https://arxiv.org/abs/2512.11727v1)** | 2025-12-12 | <details><summary>Show</summary><p>Recent advances in video analytics address real-time data drift by continuously retraining specialized, lightweight DNN models for individual cameras. However, the current practice of retraining a separate model for each camera suffers from high compute and communication costs, making it unscalable. We present ECCO, a new video analytics framework designed for resource-efficient continuous learning. The key insight is that the data drift, which necessitates model retraining, often shows temporal and spatial correlations across nearby cameras. By identifying cameras that experience similar drift and retraining a shared model for them, ECCO can substantially reduce the associated compute and communication costs. Specifically, ECCO introduces: (i) a lightweight grouping algorithm that dynamically forms and updates camera groups; (ii) a GPU allocator that dynamically assigns GPU resources across different groups to improve retraining accuracy and ensure fairness; and (iii) a transmission controller at each camera that configures frame sampling and coordinates bandwidth sharing with other cameras based on its assigned GPU resources. We conducted extensive evaluations on three distinctive datasets for two vision tasks. Compared to leading baselines, ECCO improves retraining accuracy by 6.7%-18.1% using the same compute and communication resources, or supports 3.3 times more concurrent cameras at the same accuracy.</p></details> |  |
| **[Stateless Snowflake: A Cloud-Agnostic Distributed ID Generator Using Network-Derived Identity](https://arxiv.org/abs/2512.11643v1)** | 2025-12-12 | <details><summary>Show</summary><p>Snowflake-style distributed ID generators are the industry standard for producing k-ordered, unique identifiers at scale. However, the traditional requirement for manually assigned or centrally coordinated worker IDs introduces significant friction in modern container-orchestrated environments (e.g., Kubernetes), where workloads are ephemeral and autoscaled. In such systems, maintaining stable worker identities requires complex stateful sets or external coordination services (e.g., ZooKeeper), negating the operational benefits of stateless microservices. This paper presents a cloud-agnostic, container-native ID generation protocol that eliminates the dependency on explicit worker IDs. By deriving node uniqueness deterministically from ephemeral network properties - specifically the container's private IPv4 address - the proposed method removes the need for centralized coordination. We introduce a modified bit-allocation scheme (1-41-16-6) that accommodates 16 bits of network-derived entropy while preserving strict monotonicity. We validate the approach across AWS, GCP, and Azure environments. Evaluation results demonstrate that while the design has a theoretical single-node ceiling of approximately 64,000 TPS, in practical microservice deployments the network I/O dominates latency, resulting in end-to-end performance (approximately 31,000 TPS on a 3-node cluster) comparable to classic stateful generators while offering effectively unbounded horizontal scalability.</p></details> | <details><summary>12 pa...</summary><p>12 pages, 3 tables, 1 figure</p></details> |
| **[FirecREST v2: lessons learned from redesigning an API for scalable HPC resource access](https://arxiv.org/abs/2512.11634v1)** | 2025-12-12 | <details><summary>Show</summary><p>Introducing FirecREST v2, the next generation of our open-source RESTful API for programmatic access to HPC resources. FirecREST v2 delivers a 100x performance improvement over its predecessor. This paper explores the lessons learned from redesigning FirecREST from the ground up, with a focus on integrating enhanced security and high throughput as core requirements. We provide a detailed account of our systematic performance testing methodology, highlighting common bottlenecks in proxy-based APIs with intensive I/O operations. Key design and architectural changes that enabled these performance gains are presented. Finally, we demonstrate the impact of these improvements, supported by independent peer validation, and discuss opportunities for further improvements.</p></details> | <details><summary>6 pag...</summary><p>6 pages, 2 figures. Presented at Cray User Group 2025 conference at New York, USA (May 4-8, 2025)</p></details> |
| **[Complexity at Scale: A Quantitative Analysis of an Alibaba Microservice Deployment](https://arxiv.org/abs/2504.13141v3)** | 2025-12-12 | <details><summary>Show</summary><p>Microservice management and testbed research often rests on assumptions about deployments that have rarely been validated at production scale. While recent studies have begun to characterise production microservice deployments, they are often limited in breadth, do not compare findings across deployments, and lack consideration of the implications of findings for commonly held assumptions. We analyse a distributed tracing dataset from Alibaba's production microservice deployment to examine its scale, heterogeneity, and dynamicity. By comparing our findings to prior measurements of Meta's MSA we illustrate both convergent and divergent properties, clarifying which patterns may generalise. Our study reveals extreme architectural scale, long-tail distributions of workloads and dependencies, highly diverse functionality, substantial call graph variability, and pronounced time-varying behaviour which diverge from assumptions underlying research models and testbeds. We summarise how these observations challenge common assumptions in research on fault management, scaling, and testbed design, and outline recommendations for more realistic future approaches and evaluations.</p></details> | <details><summary>25 pa...</summary><p>25 pages, 12 figures, 4 tables, revised version for conference submission</p></details> |
| **[The Ghosts of Empires: Extracting Modularity from Interleaving-Based Proofs (Extended Version)](https://arxiv.org/abs/2511.20369v3)** | 2025-12-12 | <details><summary>Show</summary><p>Implementation bugs threaten the soundness of algorithmic software verifiers. Generating correctness certificates for correct programs allows for efficient independent validation of verification results, and thus helps to reveal such bugs. Automatic generation of small, compact correctness proofs for concurrent programs is challenging, as the correctness arguments may depend on the particular interleaving, which can lead to exponential explosion. We present an approach that converts an interleaving-based correctness proof, as generated by many algorithmic verifiers, into a thread-modular correctness proof in the style of Owicki and Gries. We automatically synthesize ghost variables that capture the relevant interleaving information, and abstract away irrelevant details. Our evaluation shows that the approach is efficient in practice and generates compact proofs, compared to a baseline.</p></details> | <details><summary>39 pa...</summary><p>39 pages, 10 figures, 1 table. Extended version with proofs of the paper published at POPL'2026 (https://doi.org/10.1145/3776684) [corrections in Fig. 5 and Fig. 6]</p></details> |
| **[Context-Dependent Effects and Concurrency in Guarded Interaction Trees](https://arxiv.org/abs/2512.11577v1)** | 2025-12-12 | <details><summary>Show</summary><p>Guarded Interaction Trees are a structure and a fully formalized framework for representing higher-order computations with higher-order effects in Rocq. We present an extension of Guarded Interaction Trees to support formal reasoning about context-dependent effects. That is, effects whose behaviors depend on the evaluation context, e.g., call/cc, shift and reset. Using and reasoning about such effects is challenging since certain compositionality principles no longer hold in the presence of such effects. For example, the so-called ``bind rule'' in modern program logics is no longer valid. The goal of our extension is to support representation and reasoning about context-dependent effects in the most painless way possible. To that end, our extension is conservative: the reasoning principles for context-independent effects remain the same. We use it to give direct-style denotational semantics for higher-order programming languages with call/cc and with delimited continuations. We extend the program logic for Guarded Interaction Trees to account for context-dependent effects, and we use the program logic to prove that the denotational semantics is adequate with respect to the operational semantics. Additionally, we retain the ability to combine multiple effects in a modular way, which we demonstrate by showing type soundness for safe interoperability of a programming language with delimited continuations and a programming language with higher-order store. Furthermore, as another contribution, in addition to context-dependent effects, we show how to extend Guarded Interaction Trees with preemptive concurrency. To support implementation and verification of concurrent data structures and algorithms in the presence of preemptive concurrency one requires atomic state modification operations, e.g., compare-and-exchange.</p></details> |  |
| **[Parallax: Runtime Parallelization for Operator Fallbacks in Heterogeneous Edge Systems](https://arxiv.org/abs/2512.11532v1)** | 2025-12-12 | <details><summary>Show</summary><p>The growing demand for real-time DNN applications on edge devices necessitates faster inference of increasingly complex models. Although many devices include specialized accelerators (e.g., mobile GPUs), dynamic control-flow operators and unsupported kernels often fall back to CPU execution. Existing frameworks handle these fallbacks poorly, leaving CPU cores idle and causing high latency and memory spikes. We introduce Parallax, a framework that accelerates mobile DNN inference without model refactoring or custom operator implementations. Parallax first partitions the computation DAG to expose parallelism, then employs branch-aware memory management with dedicated arenas and buffer reuse to reduce runtime footprint. An adaptive scheduler executes branches according to device memory constraints, meanwhile, fine-grained subgraph control enables heterogeneous inference of dynamic models. By evaluating on five representative DNNs across three different mobile devices, Parallax achieves up to 46% latency reduction, maintains controlled memory overhead (26.5% on average), and delivers up to 30% energy savings compared with state-of-the-art frameworks, offering improvements aligned with the responsiveness demands of real-time mobile inference.</p></details> |  |
| **[Enhanced Pruning for Distributed Closeness Centrality under Multi-Packet Messaging](https://arxiv.org/abs/2512.11512v1)** | 2025-12-12 | <details><summary>Show</summary><p>Identifying central nodes using closeness centrality is a critical task in analyzing large-scale complex networks, yet its decentralized computation remains challenging due to high communication overhead. Existing distributed approximation techniques, such as pruning, often fail to fully mitigate the cost of exchanging numerous data packets in large network settings. In this paper, we introduce a novel enhancement to the distributed pruning method specifically designed to overcome this communication bottleneck. Our core contribution is a technique that leverages multi-packet messaging, allowing nodes to batch and transmit larger, consolidated data blocks. This approach significantly reduces the number of exchanged messages and minimizes data loss without compromising the accuracy of the centrality estimates. We demonstrate that our multi-packet approach substantially outperforms the original pruning technique in both message efficiency (fewer overall messages) and computation time, preserving the core approximation properties of the baseline method. While we observe a manageable trade-off in increased per-node memory usage and local overhead, our findings show that this is outweighed by the gains in communication efficiency, particularly for very large networks and complex packet structures. Our work offers a more scalable and efficient solution for decentralized closeness centrality computation, promising a significant step forward for large-scale network analysis.</p></details> | <details><summary>9 pag...</summary><p>9 pages, 7 figures and 3 tables. Published in a local annal at the University of Kinshasa, although the annal is not indexed</p></details> |
| **[Comparing Methods for the Cross-Level Verification of SystemC Peripherals with Symbolic Execution](https://arxiv.org/abs/2509.05504v2)** | 2025-12-12 | <details><summary>Show</summary><p>Virtual Prototypes (VPs) are important tools in modern hardware development. At high abstractions, they are often implemented in SystemC and offer early analysis of increasingly complex designs. These complex designs often combine one or more processors, interconnects, and peripherals to perform tasks in hardware or interact with the environment. Verifying these subsystems is a well-suited task for VPs, as they allow reasoning across different abstraction levels. While modern verification techniques like symbolic execution can be seamlessly integrated into VP-based workflows, they require modifications in the SystemC kernel. Hence, existing approaches modify and replace the SystemC kernel, or ignore the opportunity of cross-level scenarios completely, and would not allow focusing on special challenges of particular subsystems like peripherals. We propose CrosSym and SEFOS, two opposing approaches for a versatile symbolic execution of peripherals. CrosSym modifies the SystemC kernel, while SEFOS instead modifies a modern symbolic execution engine. Our extensive evaluation applies our tools to various peripherals on different levels of abstractions. Both tools' extensive sets of features are demonstrated for (1) different verification scenarios, and (2) identifying 300+ mutants. In comparison with each other, SEFOS convinces with the unmodified SystemC kernel and peripheral, while CrosSym offers slightly better runtime and memory usage. In comparison to the state-of-the-art, that is limited to Transaction Level Modelling (TLM), our tools offered comparable runtime, while enabling cross-level verification with symbolic execution.</p></details> |  |
| **[RollMux: Phase-Level Multiplexing for Disaggregated RL Post-Training](https://arxiv.org/abs/2512.11306v1)** | 2025-12-12 | <details><summary>Show</summary><p>Rollout-training disaggregation is emerging as the standard architecture for Reinforcement Learning (RL) post-training, where memory-bound rollout and compute-bound training are physically disaggregated onto purpose-built clusters to maximize hardware efficiency. However, the strict synchronization required by on-policy algorithms introduces severe dependency bubbles, forcing one cluster to idle while the dependent phase is running on the other. We present RollMux, a cluster scheduling framework that reclaims these bubbles through cross-cluster orchestration. RollMux is built on the insight that the structural idleness of one job can be effectively utilized by the active phase of another. To realize this, we introduce the co-execution group abstraction, which partitions the cluster into isolated locality domains. This abstraction enables a two-tier scheduling architecture: an inter-group scheduler that optimizes job placement using conservative stochastic planning, and an intra-group scheduler that orchestrates a provably optimal round-robin schedule. The group abstraction also imposes a residency constraint, ensuring that massive model states remain cached in host memory to enable "warm-star" context switching. We evaluate RollMux on a production-scale testbed with 328 H20 and 328 H800 GPUs. RollMux improves cost efficiency by 1.84x over standard disaggregation and 1.38x over state-of-the-art co-located baselines, all while achieving 100% SLO attainment.</p></details> | 17 pages, 15 figures |
| **[Aragog: Just-in-Time Model Routing for Scalable Serving of Agentic Workflows](https://arxiv.org/abs/2511.20975v2)** | 2025-12-12 | <details><summary>Show</summary><p>Agentic workflows have emerged as a powerful paradigm for solving complex, multi-stage tasks, but serving them at scale is computationally expensive given the many LLM inferences that each request must pass through. Configuration selection, or the cost-aware assignment of workflow agents to specific LLMs, can reduce these costs, but existing approaches bind configuration decisions before request execution, making them ill-suited for the heterogeneous and lengthy execution of workflows. Specifically, system loads can fluctuate rapidly and substantially during a request's lifetime, causing fixed configurations to quickly become suboptimal. We present Aragog, a system that progressively adapts a request's configuration throughout its execution to match runtime dynamics. To make this practical despite the massive space of workflow configurations, Aragog decouples the problem into two core elements -- a one-time routing step that identifies all accuracy-preserving configurations, and a cheap per-stage scheduler that selects among them using up-to-date system observations -- and introduces novel strategies to accelerate each. Across diverse workflows and model families, Aragog increases maximum serving throughput by 50.0--217.0\% and reduces median latency by 32.5--78.9\% at peak request rates, while maintaining accuracy comparable to the most expensive configurations.</p></details> |  |
| **[SSSP-Del: Fully Dynamic Distributed Algorithm for Single-Source Shortest Path](https://arxiv.org/abs/2508.14319v2)** | 2025-12-12 | <details><summary>Show</summary><p>Modern graphs are both large and dynamic, presenting significant challenges for fundamental queries, such as the Single-Source Shortest Path (SSSP) problem. Naively recomputing the SSSP tree after each topology change is prohibitively expensive, causing on-demand computation to suffer from high latency. Existing dynamic SSSP algorithms often cannot simultaneously handle both edge additions and deletions, operate in distributed memory, and provide low-latency query results. To address these challenges, this paper presents SSSP-Del, a new vertex-centric, asynchronous, and fully distributed algorithm for dynamic SSSP. Operating in a shared-nothing architecture, our algorithm processes streams of both edge insertions and deletions. We conduct a comprehensive evaluation on large real-world and synthetic graphs with millions of vertices, and provide a thorough analysis by evaluating result latency, solution stability, and throughput.</p></details> |  |
| **[GSplit: Scaling Graph Neural Network Training on Large Graphs via Split-Parallelism](https://arxiv.org/abs/2303.13775v3)** | 2025-12-12 | <details><summary>Show</summary><p>Graph neural networks (GNNs), an emerging class of machine learning models for graphs, have gained popularity for their superior performance in various graph analytical tasks. Mini-batch training is commonly used to train GNNs on large graphs, and data parallelism is the standard approach to scale mini-batch training across multiple GPUs. Data parallel approaches contain redundant work as subgraphs sampled by different GPUs contain significant overlap. To address this issue, we introduce a hybrid parallel mini-batch training paradigm called split parallelism. Split parallelism avoids redundant work by splitting the sampling, loading, and training of each mini-batch across multiple GPUs. Split parallelism, however, introduces communication overheads that can be more than the savings from removing redundant work. We further present a lightweight partitioning algorithm that probabilistically minimizes these overheads. We implement split parallelism in GSplit and show that it outperforms state-of-the-art mini-batch training systems like DGL, Quiver, and $P^3$.</p></details> | <details><summary>Publi...</summary><p>Published at MLSys 2025. OpenReview: https://openreview.net/forum?id=cTOx1YTBgh</p></details> |
| **[Theoretical Foundations of GPU-Native Compilation for Rapid Code Iteration](https://arxiv.org/abs/2512.11200v1)** | 2025-12-12 | <details><summary>Show</summary><p>Current AI code generation systems suffer from significant latency bottlenecks due to CPU-GPU data transfers during compilation, execution, and testing phases. We establish theoretical foundations for three complementary approaches to GPU-native compilation that eliminate these transfers: (1) parallel traditional compilation adapted for GPU execution, (2) neural compilation using learned sequence-to-sequence translation with probabilistic verification, and (3) hybrid architectures combining both strategies. We derive latency and energy bounds demonstrating potential speedups of 10-100x for code iteration cycles. Our analysis shows that traditional GPU compilation provides 2-5x improvements through transfer elimination, neural compilation achieves 10-100x speedups via massive parallelism, and hybrid approaches offer practical deployment paths with guaranteed correctness. We formalize the probabilistic verification framework that enables trading compilation accuracy for parallel exploration, and discuss implications for self-improving AI systems and future analog computing substrates.</p></details> | 9 pages , 2 tables |
| **[An Efficient and Adaptive Watermark Detection System with Tile-based Error Correction](https://arxiv.org/abs/2509.02447v2)** | 2025-12-12 | <details><summary>Show</summary><p>Efficient and reliable detection of generated images is critical for the responsible deployment of generative models. Existing approaches primarily focus on improving detection accuracy and robustness under various image transformations and adversarial manipulations, yet they largely overlook the efficiency challenges of watermark detection across large-scale image collections. To address this gap, we propose QRMark, an efficient and adaptive end-to-end method for detecting embedded image watermarks. The core idea of QRMark is to combine QR Code-inspired error correction with tailored tiling techniques to improve detection efficiency while preserving accuracy and robustness. At the algorithmic level, QRMark employs a Reed-Solomon error correction mechanism to mitigate the accuracy degradation introduced by tiling. At the system level, QRMark implements a resource-aware multi-channel horizontal fusion policy that adaptively assigns more streams to GPU-intensive stages of the detection pipeline. It further employs a tile-based workload interleaving strategy to overlap data-loading overhead with computation and schedules kernels across stages to maximize efficiency. End-to-end evaluations show that QRMark achieves an average 2.43x inference speedup over the sequential baseline.</p></details> |  |
| **[KVSwap: Disk-aware KV Cache Offloading for Long-Context On-device Inference](https://arxiv.org/abs/2511.11907v2)** | 2025-12-11 | <details><summary>Show</summary><p>Language models (LMs) underpin emerging mobile and embedded AI applications like meeting and video summarization and document analysis, which often require processing multiple long-context inputs. Running an LM locally on-device improves privacy, enables offline use, and reduces cost, but long-context inference quickly hits a \emph{memory capacity wall} as the key-value (KV) cache grows linearly with context length and batch size. Existing KV-cache offloading schemes are designed to transfer cache data from GPU memory to CPU memory; however, they are not suitable for embedded and mobile systems, where the CPU and GPU (or NPU) typically share a unified memory and the non-volatile secondary storage (disk) offers limited I/O bandwidth. We present KVSwap, a software framework tailored for local devices that achieves high memory efficiency while effectively leveraging disk storage. KVSwap stores the full cache on disk, uses highly compact in-memory metadata to predict which entries to preload, overlaps computation with hardware-aware disk access, and orchestrates read patterns to match storage device characteristics. Our evaluation shows that across representative LMs and storage types, KVSwap delivers higher throughput under tight memory budgets while maintaining generation quality over existing KV cache offloading schemes.</p></details> |  |

## compiler
| **Title** | **Date** | **Abstract** | **Comment** |
| --- | --- | --- | --- |
| **[Hypergraph based Multi-Party Payment Channel](https://arxiv.org/abs/2512.11775v1)** | 2025-12-12 | <details><summary>Show</summary><p>Public blockchains inherently offer low throughput and high latency, motivating off-chain scalability solutions such as Payment Channel Networks (PCNs). However, existing PCNs suffer from liquidity fragmentation-funds locked in one channel cannot be reused elsewhere-and channel depletion, both of which limit routing efficiency and reduce transaction success rates. Multi-party channel (MPC) constructions mitigate these issues, but they typically rely on leaders or coordinators, creating single points of failure and providing only limited flexibility for inter-channel payments. We introduce Hypergraph-based Multi-Party Payment Channels (H-MPCs), a new off-chain construction that replaces bilateral channels with collectively funded hyperedges. These hyperedges enable fully concurrent, leaderless intra- and inter-hyperedge payments through verifiable, proposer-ordered DAG updates, offering significantly greater flexibility and concurrency than prior designs. Our implementation on a 150-node network demonstrates a transaction success rate of approximately 94% without HTLC expiry or routing failures, highlighting the robustness of H-MPCs.</p></details> |  |
| **[The Relative Monadic Metalanguage](https://arxiv.org/abs/2512.11762v1)** | 2025-12-12 | <details><summary>Show</summary><p>Relative monads provide a controlled view of computation. We generalise the monadic metalanguage to a relative setting and give a complete semantics with strong relative monads. Adopting this perspective, we generalise two existing program calculi from the literature. We provide a linear-non-linear language for graded monads, LNL-RMM, along with a semantic proof that it is a conservative extension of the graded monadic metalanguage. Additionally, we provide a complete semantics for the arrow calculus, showing it is a restricted relative monadic metalanguage. This motivates the introduction of ARMM, a computational lambda calculus-style language for arrows that conservatively extends the arrow calculus.</p></details> | <details><summary>41 pa...</summary><p>41 pages. Published in Proceedings of the ACM on Programming Languages (POPL 2026)</p></details> |
| **[REASONING COMPILER: LLM-Guided Optimizations for Efficient Model Serving](https://arxiv.org/abs/2506.01374v4)** | 2025-12-12 | <details><summary>Show</summary><p>While model serving has unlocked unprecedented capabilities, the high cost of serving large-scale models continues to be a significant barrier to widespread accessibility and rapid innovation. Compiler optimizations have long driven substantial performance improvements, but existing compilers struggle with neural workloads due to the exponentially large and highly interdependent space of possible transformations. Although existing stochastic search techniques can be effective, they are often sample-inefficient and fail to leverage the structural context underlying compilation decisions. We set out to investigate the research question of whether reasoning with large language models (LLMs), without any retraining, can leverage the context-aware decision space of compiler optimizations to significantly improve sample efficiency. To that end, we introduce a novel compilation framework (dubbed Reasoning Compiler) that formulates optimization as a sequential, context-aware decision process guided by a large language model and structured Monte Carlo tree search (MCTS). The LLM acts as a proposal mechanism, suggesting hardware-informed transformations that reflect the current program state and accumulated performance feedback. MCTS incorporates the LLM-generated proposals to balance exploration and exploitation, facilitating structured, context-sensitive traversal of the expansive compiler optimization space. By achieving substantial speedups with markedly fewer samples than leading neural compilers, our approach demonstrates the potential of LLM-guided reasoning to transform the landscape of compiler optimization.</p></details> | NeurIPS 2025 |
| **[TD-Orch: Scalable Load-Balancing for Distributed Systems with Applications to Graph Processing](https://arxiv.org/abs/2511.11843v2)** | 2025-12-12 | <details><summary>Show</summary><p>In this paper, we introduce a task-data orchestration abstraction that supports a range of distributed applications, including graph processing and key-value stores. Given a batch of lambda tasks each requesting one or more data items, where both tasks and data are distributed across multiple machines, each task must be co-located with its target data (by moving tasks and/or data) and then executed. We present TD-Orch, an efficient and scalable orchestration framework featuring a simple application developer interface. TD-Orch employs a distributed push-pull technique, leveraging the bidirectional flow of both tasks and data to achieve scalable load balance across machines even under highly skewed data requests (data hot spots), with minimal communication overhead. Experimental results show that TD-Orch achieves up to 2.8x speedup over existing distributed scheduling baselines. Building on TD-Orch, we present TDO-GP, a distributed graph processing system for general graph problems, demonstrating the effectiveness of the underlying framework. We design three families of implementation techniques to fully leverage the execution flow provided by TD-Orch. Experimental results show that TDO-GP achieves an average speedup of 4.1x over the best prior open-source distributed graph systems for general graph processing.</p></details> |  |
| **[ECCO: Leveraging Cross-Camera Correlations for Efficient Live Video Continuous Learning](https://arxiv.org/abs/2512.11727v1)** | 2025-12-12 | <details><summary>Show</summary><p>Recent advances in video analytics address real-time data drift by continuously retraining specialized, lightweight DNN models for individual cameras. However, the current practice of retraining a separate model for each camera suffers from high compute and communication costs, making it unscalable. We present ECCO, a new video analytics framework designed for resource-efficient continuous learning. The key insight is that the data drift, which necessitates model retraining, often shows temporal and spatial correlations across nearby cameras. By identifying cameras that experience similar drift and retraining a shared model for them, ECCO can substantially reduce the associated compute and communication costs. Specifically, ECCO introduces: (i) a lightweight grouping algorithm that dynamically forms and updates camera groups; (ii) a GPU allocator that dynamically assigns GPU resources across different groups to improve retraining accuracy and ensure fairness; and (iii) a transmission controller at each camera that configures frame sampling and coordinates bandwidth sharing with other cameras based on its assigned GPU resources. We conducted extensive evaluations on three distinctive datasets for two vision tasks. Compared to leading baselines, ECCO improves retraining accuracy by 6.7%-18.1% using the same compute and communication resources, or supports 3.3 times more concurrent cameras at the same accuracy.</p></details> |  |
| **[Stateless Snowflake: A Cloud-Agnostic Distributed ID Generator Using Network-Derived Identity](https://arxiv.org/abs/2512.11643v1)** | 2025-12-12 | <details><summary>Show</summary><p>Snowflake-style distributed ID generators are the industry standard for producing k-ordered, unique identifiers at scale. However, the traditional requirement for manually assigned or centrally coordinated worker IDs introduces significant friction in modern container-orchestrated environments (e.g., Kubernetes), where workloads are ephemeral and autoscaled. In such systems, maintaining stable worker identities requires complex stateful sets or external coordination services (e.g., ZooKeeper), negating the operational benefits of stateless microservices. This paper presents a cloud-agnostic, container-native ID generation protocol that eliminates the dependency on explicit worker IDs. By deriving node uniqueness deterministically from ephemeral network properties - specifically the container's private IPv4 address - the proposed method removes the need for centralized coordination. We introduce a modified bit-allocation scheme (1-41-16-6) that accommodates 16 bits of network-derived entropy while preserving strict monotonicity. We validate the approach across AWS, GCP, and Azure environments. Evaluation results demonstrate that while the design has a theoretical single-node ceiling of approximately 64,000 TPS, in practical microservice deployments the network I/O dominates latency, resulting in end-to-end performance (approximately 31,000 TPS on a 3-node cluster) comparable to classic stateful generators while offering effectively unbounded horizontal scalability.</p></details> | <details><summary>12 pa...</summary><p>12 pages, 3 tables, 1 figure</p></details> |
| **[FirecREST v2: lessons learned from redesigning an API for scalable HPC resource access](https://arxiv.org/abs/2512.11634v1)** | 2025-12-12 | <details><summary>Show</summary><p>Introducing FirecREST v2, the next generation of our open-source RESTful API for programmatic access to HPC resources. FirecREST v2 delivers a 100x performance improvement over its predecessor. This paper explores the lessons learned from redesigning FirecREST from the ground up, with a focus on integrating enhanced security and high throughput as core requirements. We provide a detailed account of our systematic performance testing methodology, highlighting common bottlenecks in proxy-based APIs with intensive I/O operations. Key design and architectural changes that enabled these performance gains are presented. Finally, we demonstrate the impact of these improvements, supported by independent peer validation, and discuss opportunities for further improvements.</p></details> | <details><summary>6 pag...</summary><p>6 pages, 2 figures. Presented at Cray User Group 2025 conference at New York, USA (May 4-8, 2025)</p></details> |
| **[Complexity at Scale: A Quantitative Analysis of an Alibaba Microservice Deployment](https://arxiv.org/abs/2504.13141v3)** | 2025-12-12 | <details><summary>Show</summary><p>Microservice management and testbed research often rests on assumptions about deployments that have rarely been validated at production scale. While recent studies have begun to characterise production microservice deployments, they are often limited in breadth, do not compare findings across deployments, and lack consideration of the implications of findings for commonly held assumptions. We analyse a distributed tracing dataset from Alibaba's production microservice deployment to examine its scale, heterogeneity, and dynamicity. By comparing our findings to prior measurements of Meta's MSA we illustrate both convergent and divergent properties, clarifying which patterns may generalise. Our study reveals extreme architectural scale, long-tail distributions of workloads and dependencies, highly diverse functionality, substantial call graph variability, and pronounced time-varying behaviour which diverge from assumptions underlying research models and testbeds. We summarise how these observations challenge common assumptions in research on fault management, scaling, and testbed design, and outline recommendations for more realistic future approaches and evaluations.</p></details> | <details><summary>25 pa...</summary><p>25 pages, 12 figures, 4 tables, revised version for conference submission</p></details> |
| **[The Ghosts of Empires: Extracting Modularity from Interleaving-Based Proofs (Extended Version)](https://arxiv.org/abs/2511.20369v3)** | 2025-12-12 | <details><summary>Show</summary><p>Implementation bugs threaten the soundness of algorithmic software verifiers. Generating correctness certificates for correct programs allows for efficient independent validation of verification results, and thus helps to reveal such bugs. Automatic generation of small, compact correctness proofs for concurrent programs is challenging, as the correctness arguments may depend on the particular interleaving, which can lead to exponential explosion. We present an approach that converts an interleaving-based correctness proof, as generated by many algorithmic verifiers, into a thread-modular correctness proof in the style of Owicki and Gries. We automatically synthesize ghost variables that capture the relevant interleaving information, and abstract away irrelevant details. Our evaluation shows that the approach is efficient in practice and generates compact proofs, compared to a baseline.</p></details> | <details><summary>39 pa...</summary><p>39 pages, 10 figures, 1 table. Extended version with proofs of the paper published at POPL'2026 (https://doi.org/10.1145/3776684) [corrections in Fig. 5 and Fig. 6]</p></details> |
| **[Context-Dependent Effects and Concurrency in Guarded Interaction Trees](https://arxiv.org/abs/2512.11577v1)** | 2025-12-12 | <details><summary>Show</summary><p>Guarded Interaction Trees are a structure and a fully formalized framework for representing higher-order computations with higher-order effects in Rocq. We present an extension of Guarded Interaction Trees to support formal reasoning about context-dependent effects. That is, effects whose behaviors depend on the evaluation context, e.g., call/cc, shift and reset. Using and reasoning about such effects is challenging since certain compositionality principles no longer hold in the presence of such effects. For example, the so-called ``bind rule'' in modern program logics is no longer valid. The goal of our extension is to support representation and reasoning about context-dependent effects in the most painless way possible. To that end, our extension is conservative: the reasoning principles for context-independent effects remain the same. We use it to give direct-style denotational semantics for higher-order programming languages with call/cc and with delimited continuations. We extend the program logic for Guarded Interaction Trees to account for context-dependent effects, and we use the program logic to prove that the denotational semantics is adequate with respect to the operational semantics. Additionally, we retain the ability to combine multiple effects in a modular way, which we demonstrate by showing type soundness for safe interoperability of a programming language with delimited continuations and a programming language with higher-order store. Furthermore, as another contribution, in addition to context-dependent effects, we show how to extend Guarded Interaction Trees with preemptive concurrency. To support implementation and verification of concurrent data structures and algorithms in the presence of preemptive concurrency one requires atomic state modification operations, e.g., compare-and-exchange.</p></details> |  |
| **[Parallax: Runtime Parallelization for Operator Fallbacks in Heterogeneous Edge Systems](https://arxiv.org/abs/2512.11532v1)** | 2025-12-12 | <details><summary>Show</summary><p>The growing demand for real-time DNN applications on edge devices necessitates faster inference of increasingly complex models. Although many devices include specialized accelerators (e.g., mobile GPUs), dynamic control-flow operators and unsupported kernels often fall back to CPU execution. Existing frameworks handle these fallbacks poorly, leaving CPU cores idle and causing high latency and memory spikes. We introduce Parallax, a framework that accelerates mobile DNN inference without model refactoring or custom operator implementations. Parallax first partitions the computation DAG to expose parallelism, then employs branch-aware memory management with dedicated arenas and buffer reuse to reduce runtime footprint. An adaptive scheduler executes branches according to device memory constraints, meanwhile, fine-grained subgraph control enables heterogeneous inference of dynamic models. By evaluating on five representative DNNs across three different mobile devices, Parallax achieves up to 46% latency reduction, maintains controlled memory overhead (26.5% on average), and delivers up to 30% energy savings compared with state-of-the-art frameworks, offering improvements aligned with the responsiveness demands of real-time mobile inference.</p></details> |  |
| **[Enhanced Pruning for Distributed Closeness Centrality under Multi-Packet Messaging](https://arxiv.org/abs/2512.11512v1)** | 2025-12-12 | <details><summary>Show</summary><p>Identifying central nodes using closeness centrality is a critical task in analyzing large-scale complex networks, yet its decentralized computation remains challenging due to high communication overhead. Existing distributed approximation techniques, such as pruning, often fail to fully mitigate the cost of exchanging numerous data packets in large network settings. In this paper, we introduce a novel enhancement to the distributed pruning method specifically designed to overcome this communication bottleneck. Our core contribution is a technique that leverages multi-packet messaging, allowing nodes to batch and transmit larger, consolidated data blocks. This approach significantly reduces the number of exchanged messages and minimizes data loss without compromising the accuracy of the centrality estimates. We demonstrate that our multi-packet approach substantially outperforms the original pruning technique in both message efficiency (fewer overall messages) and computation time, preserving the core approximation properties of the baseline method. While we observe a manageable trade-off in increased per-node memory usage and local overhead, our findings show that this is outweighed by the gains in communication efficiency, particularly for very large networks and complex packet structures. Our work offers a more scalable and efficient solution for decentralized closeness centrality computation, promising a significant step forward for large-scale network analysis.</p></details> | <details><summary>9 pag...</summary><p>9 pages, 7 figures and 3 tables. Published in a local annal at the University of Kinshasa, although the annal is not indexed</p></details> |
| **[Comparing Methods for the Cross-Level Verification of SystemC Peripherals with Symbolic Execution](https://arxiv.org/abs/2509.05504v2)** | 2025-12-12 | <details><summary>Show</summary><p>Virtual Prototypes (VPs) are important tools in modern hardware development. At high abstractions, they are often implemented in SystemC and offer early analysis of increasingly complex designs. These complex designs often combine one or more processors, interconnects, and peripherals to perform tasks in hardware or interact with the environment. Verifying these subsystems is a well-suited task for VPs, as they allow reasoning across different abstraction levels. While modern verification techniques like symbolic execution can be seamlessly integrated into VP-based workflows, they require modifications in the SystemC kernel. Hence, existing approaches modify and replace the SystemC kernel, or ignore the opportunity of cross-level scenarios completely, and would not allow focusing on special challenges of particular subsystems like peripherals. We propose CrosSym and SEFOS, two opposing approaches for a versatile symbolic execution of peripherals. CrosSym modifies the SystemC kernel, while SEFOS instead modifies a modern symbolic execution engine. Our extensive evaluation applies our tools to various peripherals on different levels of abstractions. Both tools' extensive sets of features are demonstrated for (1) different verification scenarios, and (2) identifying 300+ mutants. In comparison with each other, SEFOS convinces with the unmodified SystemC kernel and peripheral, while CrosSym offers slightly better runtime and memory usage. In comparison to the state-of-the-art, that is limited to Transaction Level Modelling (TLM), our tools offered comparable runtime, while enabling cross-level verification with symbolic execution.</p></details> |  |
| **[RollMux: Phase-Level Multiplexing for Disaggregated RL Post-Training](https://arxiv.org/abs/2512.11306v1)** | 2025-12-12 | <details><summary>Show</summary><p>Rollout-training disaggregation is emerging as the standard architecture for Reinforcement Learning (RL) post-training, where memory-bound rollout and compute-bound training are physically disaggregated onto purpose-built clusters to maximize hardware efficiency. However, the strict synchronization required by on-policy algorithms introduces severe dependency bubbles, forcing one cluster to idle while the dependent phase is running on the other. We present RollMux, a cluster scheduling framework that reclaims these bubbles through cross-cluster orchestration. RollMux is built on the insight that the structural idleness of one job can be effectively utilized by the active phase of another. To realize this, we introduce the co-execution group abstraction, which partitions the cluster into isolated locality domains. This abstraction enables a two-tier scheduling architecture: an inter-group scheduler that optimizes job placement using conservative stochastic planning, and an intra-group scheduler that orchestrates a provably optimal round-robin schedule. The group abstraction also imposes a residency constraint, ensuring that massive model states remain cached in host memory to enable "warm-star" context switching. We evaluate RollMux on a production-scale testbed with 328 H20 and 328 H800 GPUs. RollMux improves cost efficiency by 1.84x over standard disaggregation and 1.38x over state-of-the-art co-located baselines, all while achieving 100% SLO attainment.</p></details> | 17 pages, 15 figures |
| **[Aragog: Just-in-Time Model Routing for Scalable Serving of Agentic Workflows](https://arxiv.org/abs/2511.20975v2)** | 2025-12-12 | <details><summary>Show</summary><p>Agentic workflows have emerged as a powerful paradigm for solving complex, multi-stage tasks, but serving them at scale is computationally expensive given the many LLM inferences that each request must pass through. Configuration selection, or the cost-aware assignment of workflow agents to specific LLMs, can reduce these costs, but existing approaches bind configuration decisions before request execution, making them ill-suited for the heterogeneous and lengthy execution of workflows. Specifically, system loads can fluctuate rapidly and substantially during a request's lifetime, causing fixed configurations to quickly become suboptimal. We present Aragog, a system that progressively adapts a request's configuration throughout its execution to match runtime dynamics. To make this practical despite the massive space of workflow configurations, Aragog decouples the problem into two core elements -- a one-time routing step that identifies all accuracy-preserving configurations, and a cheap per-stage scheduler that selects among them using up-to-date system observations -- and introduces novel strategies to accelerate each. Across diverse workflows and model families, Aragog increases maximum serving throughput by 50.0--217.0\% and reduces median latency by 32.5--78.9\% at peak request rates, while maintaining accuracy comparable to the most expensive configurations.</p></details> |  |
| **[SSSP-Del: Fully Dynamic Distributed Algorithm for Single-Source Shortest Path](https://arxiv.org/abs/2508.14319v2)** | 2025-12-12 | <details><summary>Show</summary><p>Modern graphs are both large and dynamic, presenting significant challenges for fundamental queries, such as the Single-Source Shortest Path (SSSP) problem. Naively recomputing the SSSP tree after each topology change is prohibitively expensive, causing on-demand computation to suffer from high latency. Existing dynamic SSSP algorithms often cannot simultaneously handle both edge additions and deletions, operate in distributed memory, and provide low-latency query results. To address these challenges, this paper presents SSSP-Del, a new vertex-centric, asynchronous, and fully distributed algorithm for dynamic SSSP. Operating in a shared-nothing architecture, our algorithm processes streams of both edge insertions and deletions. We conduct a comprehensive evaluation on large real-world and synthetic graphs with millions of vertices, and provide a thorough analysis by evaluating result latency, solution stability, and throughput.</p></details> |  |
| **[GSplit: Scaling Graph Neural Network Training on Large Graphs via Split-Parallelism](https://arxiv.org/abs/2303.13775v3)** | 2025-12-12 | <details><summary>Show</summary><p>Graph neural networks (GNNs), an emerging class of machine learning models for graphs, have gained popularity for their superior performance in various graph analytical tasks. Mini-batch training is commonly used to train GNNs on large graphs, and data parallelism is the standard approach to scale mini-batch training across multiple GPUs. Data parallel approaches contain redundant work as subgraphs sampled by different GPUs contain significant overlap. To address this issue, we introduce a hybrid parallel mini-batch training paradigm called split parallelism. Split parallelism avoids redundant work by splitting the sampling, loading, and training of each mini-batch across multiple GPUs. Split parallelism, however, introduces communication overheads that can be more than the savings from removing redundant work. We further present a lightweight partitioning algorithm that probabilistically minimizes these overheads. We implement split parallelism in GSplit and show that it outperforms state-of-the-art mini-batch training systems like DGL, Quiver, and $P^3$.</p></details> | <details><summary>Publi...</summary><p>Published at MLSys 2025. OpenReview: https://openreview.net/forum?id=cTOx1YTBgh</p></details> |
| **[Theoretical Foundations of GPU-Native Compilation for Rapid Code Iteration](https://arxiv.org/abs/2512.11200v1)** | 2025-12-12 | <details><summary>Show</summary><p>Current AI code generation systems suffer from significant latency bottlenecks due to CPU-GPU data transfers during compilation, execution, and testing phases. We establish theoretical foundations for three complementary approaches to GPU-native compilation that eliminate these transfers: (1) parallel traditional compilation adapted for GPU execution, (2) neural compilation using learned sequence-to-sequence translation with probabilistic verification, and (3) hybrid architectures combining both strategies. We derive latency and energy bounds demonstrating potential speedups of 10-100x for code iteration cycles. Our analysis shows that traditional GPU compilation provides 2-5x improvements through transfer elimination, neural compilation achieves 10-100x speedups via massive parallelism, and hybrid approaches offer practical deployment paths with guaranteed correctness. We formalize the probabilistic verification framework that enables trading compilation accuracy for parallel exploration, and discuss implications for self-improving AI systems and future analog computing substrates.</p></details> | 9 pages , 2 tables |
| **[An Efficient and Adaptive Watermark Detection System with Tile-based Error Correction](https://arxiv.org/abs/2509.02447v2)** | 2025-12-12 | <details><summary>Show</summary><p>Efficient and reliable detection of generated images is critical for the responsible deployment of generative models. Existing approaches primarily focus on improving detection accuracy and robustness under various image transformations and adversarial manipulations, yet they largely overlook the efficiency challenges of watermark detection across large-scale image collections. To address this gap, we propose QRMark, an efficient and adaptive end-to-end method for detecting embedded image watermarks. The core idea of QRMark is to combine QR Code-inspired error correction with tailored tiling techniques to improve detection efficiency while preserving accuracy and robustness. At the algorithmic level, QRMark employs a Reed-Solomon error correction mechanism to mitigate the accuracy degradation introduced by tiling. At the system level, QRMark implements a resource-aware multi-channel horizontal fusion policy that adaptively assigns more streams to GPU-intensive stages of the detection pipeline. It further employs a tile-based workload interleaving strategy to overlap data-loading overhead with computation and schedules kernels across stages to maximize efficiency. End-to-end evaluations show that QRMark achieves an average 2.43x inference speedup over the sequential baseline.</p></details> |  |
| **[KVSwap: Disk-aware KV Cache Offloading for Long-Context On-device Inference](https://arxiv.org/abs/2511.11907v2)** | 2025-12-11 | <details><summary>Show</summary><p>Language models (LMs) underpin emerging mobile and embedded AI applications like meeting and video summarization and document analysis, which often require processing multiple long-context inputs. Running an LM locally on-device improves privacy, enables offline use, and reduces cost, but long-context inference quickly hits a \emph{memory capacity wall} as the key-value (KV) cache grows linearly with context length and batch size. Existing KV-cache offloading schemes are designed to transfer cache data from GPU memory to CPU memory; however, they are not suitable for embedded and mobile systems, where the CPU and GPU (or NPU) typically share a unified memory and the non-volatile secondary storage (disk) offers limited I/O bandwidth. We present KVSwap, a software framework tailored for local devices that achieves high memory efficiency while effectively leveraging disk storage. KVSwap stores the full cache on disk, uses highly compact in-memory metadata to predict which entries to preload, overlaps computation with hardware-aware disk access, and orchestrates read patterns to match storage device characteristics. Our evaluation shows that across representative LMs and storage types, KVSwap delivers higher throughput under tight memory budgets while maintaining generation quality over existing KV cache offloading schemes.</p></details> |  |

## performance
| **Title** | **Date** | **Abstract** | **Comment** |
| --- | --- | --- | --- |
| **[Hypergraph based Multi-Party Payment Channel](https://arxiv.org/abs/2512.11775v1)** | 2025-12-12 | <details><summary>Show</summary><p>Public blockchains inherently offer low throughput and high latency, motivating off-chain scalability solutions such as Payment Channel Networks (PCNs). However, existing PCNs suffer from liquidity fragmentation-funds locked in one channel cannot be reused elsewhere-and channel depletion, both of which limit routing efficiency and reduce transaction success rates. Multi-party channel (MPC) constructions mitigate these issues, but they typically rely on leaders or coordinators, creating single points of failure and providing only limited flexibility for inter-channel payments. We introduce Hypergraph-based Multi-Party Payment Channels (H-MPCs), a new off-chain construction that replaces bilateral channels with collectively funded hyperedges. These hyperedges enable fully concurrent, leaderless intra- and inter-hyperedge payments through verifiable, proposer-ordered DAG updates, offering significantly greater flexibility and concurrency than prior designs. Our implementation on a 150-node network demonstrates a transaction success rate of approximately 94% without HTLC expiry or routing failures, highlighting the robustness of H-MPCs.</p></details> |  |
| **[The Relative Monadic Metalanguage](https://arxiv.org/abs/2512.11762v1)** | 2025-12-12 | <details><summary>Show</summary><p>Relative monads provide a controlled view of computation. We generalise the monadic metalanguage to a relative setting and give a complete semantics with strong relative monads. Adopting this perspective, we generalise two existing program calculi from the literature. We provide a linear-non-linear language for graded monads, LNL-RMM, along with a semantic proof that it is a conservative extension of the graded monadic metalanguage. Additionally, we provide a complete semantics for the arrow calculus, showing it is a restricted relative monadic metalanguage. This motivates the introduction of ARMM, a computational lambda calculus-style language for arrows that conservatively extends the arrow calculus.</p></details> | <details><summary>41 pa...</summary><p>41 pages. Published in Proceedings of the ACM on Programming Languages (POPL 2026)</p></details> |
| **[REASONING COMPILER: LLM-Guided Optimizations for Efficient Model Serving](https://arxiv.org/abs/2506.01374v4)** | 2025-12-12 | <details><summary>Show</summary><p>While model serving has unlocked unprecedented capabilities, the high cost of serving large-scale models continues to be a significant barrier to widespread accessibility and rapid innovation. Compiler optimizations have long driven substantial performance improvements, but existing compilers struggle with neural workloads due to the exponentially large and highly interdependent space of possible transformations. Although existing stochastic search techniques can be effective, they are often sample-inefficient and fail to leverage the structural context underlying compilation decisions. We set out to investigate the research question of whether reasoning with large language models (LLMs), without any retraining, can leverage the context-aware decision space of compiler optimizations to significantly improve sample efficiency. To that end, we introduce a novel compilation framework (dubbed Reasoning Compiler) that formulates optimization as a sequential, context-aware decision process guided by a large language model and structured Monte Carlo tree search (MCTS). The LLM acts as a proposal mechanism, suggesting hardware-informed transformations that reflect the current program state and accumulated performance feedback. MCTS incorporates the LLM-generated proposals to balance exploration and exploitation, facilitating structured, context-sensitive traversal of the expansive compiler optimization space. By achieving substantial speedups with markedly fewer samples than leading neural compilers, our approach demonstrates the potential of LLM-guided reasoning to transform the landscape of compiler optimization.</p></details> | NeurIPS 2025 |
| **[TD-Orch: Scalable Load-Balancing for Distributed Systems with Applications to Graph Processing](https://arxiv.org/abs/2511.11843v2)** | 2025-12-12 | <details><summary>Show</summary><p>In this paper, we introduce a task-data orchestration abstraction that supports a range of distributed applications, including graph processing and key-value stores. Given a batch of lambda tasks each requesting one or more data items, where both tasks and data are distributed across multiple machines, each task must be co-located with its target data (by moving tasks and/or data) and then executed. We present TD-Orch, an efficient and scalable orchestration framework featuring a simple application developer interface. TD-Orch employs a distributed push-pull technique, leveraging the bidirectional flow of both tasks and data to achieve scalable load balance across machines even under highly skewed data requests (data hot spots), with minimal communication overhead. Experimental results show that TD-Orch achieves up to 2.8x speedup over existing distributed scheduling baselines. Building on TD-Orch, we present TDO-GP, a distributed graph processing system for general graph problems, demonstrating the effectiveness of the underlying framework. We design three families of implementation techniques to fully leverage the execution flow provided by TD-Orch. Experimental results show that TDO-GP achieves an average speedup of 4.1x over the best prior open-source distributed graph systems for general graph processing.</p></details> |  |
| **[ECCO: Leveraging Cross-Camera Correlations for Efficient Live Video Continuous Learning](https://arxiv.org/abs/2512.11727v1)** | 2025-12-12 | <details><summary>Show</summary><p>Recent advances in video analytics address real-time data drift by continuously retraining specialized, lightweight DNN models for individual cameras. However, the current practice of retraining a separate model for each camera suffers from high compute and communication costs, making it unscalable. We present ECCO, a new video analytics framework designed for resource-efficient continuous learning. The key insight is that the data drift, which necessitates model retraining, often shows temporal and spatial correlations across nearby cameras. By identifying cameras that experience similar drift and retraining a shared model for them, ECCO can substantially reduce the associated compute and communication costs. Specifically, ECCO introduces: (i) a lightweight grouping algorithm that dynamically forms and updates camera groups; (ii) a GPU allocator that dynamically assigns GPU resources across different groups to improve retraining accuracy and ensure fairness; and (iii) a transmission controller at each camera that configures frame sampling and coordinates bandwidth sharing with other cameras based on its assigned GPU resources. We conducted extensive evaluations on three distinctive datasets for two vision tasks. Compared to leading baselines, ECCO improves retraining accuracy by 6.7%-18.1% using the same compute and communication resources, or supports 3.3 times more concurrent cameras at the same accuracy.</p></details> |  |
| **[Stateless Snowflake: A Cloud-Agnostic Distributed ID Generator Using Network-Derived Identity](https://arxiv.org/abs/2512.11643v1)** | 2025-12-12 | <details><summary>Show</summary><p>Snowflake-style distributed ID generators are the industry standard for producing k-ordered, unique identifiers at scale. However, the traditional requirement for manually assigned or centrally coordinated worker IDs introduces significant friction in modern container-orchestrated environments (e.g., Kubernetes), where workloads are ephemeral and autoscaled. In such systems, maintaining stable worker identities requires complex stateful sets or external coordination services (e.g., ZooKeeper), negating the operational benefits of stateless microservices. This paper presents a cloud-agnostic, container-native ID generation protocol that eliminates the dependency on explicit worker IDs. By deriving node uniqueness deterministically from ephemeral network properties - specifically the container's private IPv4 address - the proposed method removes the need for centralized coordination. We introduce a modified bit-allocation scheme (1-41-16-6) that accommodates 16 bits of network-derived entropy while preserving strict monotonicity. We validate the approach across AWS, GCP, and Azure environments. Evaluation results demonstrate that while the design has a theoretical single-node ceiling of approximately 64,000 TPS, in practical microservice deployments the network I/O dominates latency, resulting in end-to-end performance (approximately 31,000 TPS on a 3-node cluster) comparable to classic stateful generators while offering effectively unbounded horizontal scalability.</p></details> | <details><summary>12 pa...</summary><p>12 pages, 3 tables, 1 figure</p></details> |
| **[FirecREST v2: lessons learned from redesigning an API for scalable HPC resource access](https://arxiv.org/abs/2512.11634v1)** | 2025-12-12 | <details><summary>Show</summary><p>Introducing FirecREST v2, the next generation of our open-source RESTful API for programmatic access to HPC resources. FirecREST v2 delivers a 100x performance improvement over its predecessor. This paper explores the lessons learned from redesigning FirecREST from the ground up, with a focus on integrating enhanced security and high throughput as core requirements. We provide a detailed account of our systematic performance testing methodology, highlighting common bottlenecks in proxy-based APIs with intensive I/O operations. Key design and architectural changes that enabled these performance gains are presented. Finally, we demonstrate the impact of these improvements, supported by independent peer validation, and discuss opportunities for further improvements.</p></details> | <details><summary>6 pag...</summary><p>6 pages, 2 figures. Presented at Cray User Group 2025 conference at New York, USA (May 4-8, 2025)</p></details> |
| **[Complexity at Scale: A Quantitative Analysis of an Alibaba Microservice Deployment](https://arxiv.org/abs/2504.13141v3)** | 2025-12-12 | <details><summary>Show</summary><p>Microservice management and testbed research often rests on assumptions about deployments that have rarely been validated at production scale. While recent studies have begun to characterise production microservice deployments, they are often limited in breadth, do not compare findings across deployments, and lack consideration of the implications of findings for commonly held assumptions. We analyse a distributed tracing dataset from Alibaba's production microservice deployment to examine its scale, heterogeneity, and dynamicity. By comparing our findings to prior measurements of Meta's MSA we illustrate both convergent and divergent properties, clarifying which patterns may generalise. Our study reveals extreme architectural scale, long-tail distributions of workloads and dependencies, highly diverse functionality, substantial call graph variability, and pronounced time-varying behaviour which diverge from assumptions underlying research models and testbeds. We summarise how these observations challenge common assumptions in research on fault management, scaling, and testbed design, and outline recommendations for more realistic future approaches and evaluations.</p></details> | <details><summary>25 pa...</summary><p>25 pages, 12 figures, 4 tables, revised version for conference submission</p></details> |
| **[The Ghosts of Empires: Extracting Modularity from Interleaving-Based Proofs (Extended Version)](https://arxiv.org/abs/2511.20369v3)** | 2025-12-12 | <details><summary>Show</summary><p>Implementation bugs threaten the soundness of algorithmic software verifiers. Generating correctness certificates for correct programs allows for efficient independent validation of verification results, and thus helps to reveal such bugs. Automatic generation of small, compact correctness proofs for concurrent programs is challenging, as the correctness arguments may depend on the particular interleaving, which can lead to exponential explosion. We present an approach that converts an interleaving-based correctness proof, as generated by many algorithmic verifiers, into a thread-modular correctness proof in the style of Owicki and Gries. We automatically synthesize ghost variables that capture the relevant interleaving information, and abstract away irrelevant details. Our evaluation shows that the approach is efficient in practice and generates compact proofs, compared to a baseline.</p></details> | <details><summary>39 pa...</summary><p>39 pages, 10 figures, 1 table. Extended version with proofs of the paper published at POPL'2026 (https://doi.org/10.1145/3776684) [corrections in Fig. 5 and Fig. 6]</p></details> |
| **[Context-Dependent Effects and Concurrency in Guarded Interaction Trees](https://arxiv.org/abs/2512.11577v1)** | 2025-12-12 | <details><summary>Show</summary><p>Guarded Interaction Trees are a structure and a fully formalized framework for representing higher-order computations with higher-order effects in Rocq. We present an extension of Guarded Interaction Trees to support formal reasoning about context-dependent effects. That is, effects whose behaviors depend on the evaluation context, e.g., call/cc, shift and reset. Using and reasoning about such effects is challenging since certain compositionality principles no longer hold in the presence of such effects. For example, the so-called ``bind rule'' in modern program logics is no longer valid. The goal of our extension is to support representation and reasoning about context-dependent effects in the most painless way possible. To that end, our extension is conservative: the reasoning principles for context-independent effects remain the same. We use it to give direct-style denotational semantics for higher-order programming languages with call/cc and with delimited continuations. We extend the program logic for Guarded Interaction Trees to account for context-dependent effects, and we use the program logic to prove that the denotational semantics is adequate with respect to the operational semantics. Additionally, we retain the ability to combine multiple effects in a modular way, which we demonstrate by showing type soundness for safe interoperability of a programming language with delimited continuations and a programming language with higher-order store. Furthermore, as another contribution, in addition to context-dependent effects, we show how to extend Guarded Interaction Trees with preemptive concurrency. To support implementation and verification of concurrent data structures and algorithms in the presence of preemptive concurrency one requires atomic state modification operations, e.g., compare-and-exchange.</p></details> |  |
| **[Parallax: Runtime Parallelization for Operator Fallbacks in Heterogeneous Edge Systems](https://arxiv.org/abs/2512.11532v1)** | 2025-12-12 | <details><summary>Show</summary><p>The growing demand for real-time DNN applications on edge devices necessitates faster inference of increasingly complex models. Although many devices include specialized accelerators (e.g., mobile GPUs), dynamic control-flow operators and unsupported kernels often fall back to CPU execution. Existing frameworks handle these fallbacks poorly, leaving CPU cores idle and causing high latency and memory spikes. We introduce Parallax, a framework that accelerates mobile DNN inference without model refactoring or custom operator implementations. Parallax first partitions the computation DAG to expose parallelism, then employs branch-aware memory management with dedicated arenas and buffer reuse to reduce runtime footprint. An adaptive scheduler executes branches according to device memory constraints, meanwhile, fine-grained subgraph control enables heterogeneous inference of dynamic models. By evaluating on five representative DNNs across three different mobile devices, Parallax achieves up to 46% latency reduction, maintains controlled memory overhead (26.5% on average), and delivers up to 30% energy savings compared with state-of-the-art frameworks, offering improvements aligned with the responsiveness demands of real-time mobile inference.</p></details> |  |
| **[Enhanced Pruning for Distributed Closeness Centrality under Multi-Packet Messaging](https://arxiv.org/abs/2512.11512v1)** | 2025-12-12 | <details><summary>Show</summary><p>Identifying central nodes using closeness centrality is a critical task in analyzing large-scale complex networks, yet its decentralized computation remains challenging due to high communication overhead. Existing distributed approximation techniques, such as pruning, often fail to fully mitigate the cost of exchanging numerous data packets in large network settings. In this paper, we introduce a novel enhancement to the distributed pruning method specifically designed to overcome this communication bottleneck. Our core contribution is a technique that leverages multi-packet messaging, allowing nodes to batch and transmit larger, consolidated data blocks. This approach significantly reduces the number of exchanged messages and minimizes data loss without compromising the accuracy of the centrality estimates. We demonstrate that our multi-packet approach substantially outperforms the original pruning technique in both message efficiency (fewer overall messages) and computation time, preserving the core approximation properties of the baseline method. While we observe a manageable trade-off in increased per-node memory usage and local overhead, our findings show that this is outweighed by the gains in communication efficiency, particularly for very large networks and complex packet structures. Our work offers a more scalable and efficient solution for decentralized closeness centrality computation, promising a significant step forward for large-scale network analysis.</p></details> | <details><summary>9 pag...</summary><p>9 pages, 7 figures and 3 tables. Published in a local annal at the University of Kinshasa, although the annal is not indexed</p></details> |
| **[Comparing Methods for the Cross-Level Verification of SystemC Peripherals with Symbolic Execution](https://arxiv.org/abs/2509.05504v2)** | 2025-12-12 | <details><summary>Show</summary><p>Virtual Prototypes (VPs) are important tools in modern hardware development. At high abstractions, they are often implemented in SystemC and offer early analysis of increasingly complex designs. These complex designs often combine one or more processors, interconnects, and peripherals to perform tasks in hardware or interact with the environment. Verifying these subsystems is a well-suited task for VPs, as they allow reasoning across different abstraction levels. While modern verification techniques like symbolic execution can be seamlessly integrated into VP-based workflows, they require modifications in the SystemC kernel. Hence, existing approaches modify and replace the SystemC kernel, or ignore the opportunity of cross-level scenarios completely, and would not allow focusing on special challenges of particular subsystems like peripherals. We propose CrosSym and SEFOS, two opposing approaches for a versatile symbolic execution of peripherals. CrosSym modifies the SystemC kernel, while SEFOS instead modifies a modern symbolic execution engine. Our extensive evaluation applies our tools to various peripherals on different levels of abstractions. Both tools' extensive sets of features are demonstrated for (1) different verification scenarios, and (2) identifying 300+ mutants. In comparison with each other, SEFOS convinces with the unmodified SystemC kernel and peripheral, while CrosSym offers slightly better runtime and memory usage. In comparison to the state-of-the-art, that is limited to Transaction Level Modelling (TLM), our tools offered comparable runtime, while enabling cross-level verification with symbolic execution.</p></details> |  |
| **[RollMux: Phase-Level Multiplexing for Disaggregated RL Post-Training](https://arxiv.org/abs/2512.11306v1)** | 2025-12-12 | <details><summary>Show</summary><p>Rollout-training disaggregation is emerging as the standard architecture for Reinforcement Learning (RL) post-training, where memory-bound rollout and compute-bound training are physically disaggregated onto purpose-built clusters to maximize hardware efficiency. However, the strict synchronization required by on-policy algorithms introduces severe dependency bubbles, forcing one cluster to idle while the dependent phase is running on the other. We present RollMux, a cluster scheduling framework that reclaims these bubbles through cross-cluster orchestration. RollMux is built on the insight that the structural idleness of one job can be effectively utilized by the active phase of another. To realize this, we introduce the co-execution group abstraction, which partitions the cluster into isolated locality domains. This abstraction enables a two-tier scheduling architecture: an inter-group scheduler that optimizes job placement using conservative stochastic planning, and an intra-group scheduler that orchestrates a provably optimal round-robin schedule. The group abstraction also imposes a residency constraint, ensuring that massive model states remain cached in host memory to enable "warm-star" context switching. We evaluate RollMux on a production-scale testbed with 328 H20 and 328 H800 GPUs. RollMux improves cost efficiency by 1.84x over standard disaggregation and 1.38x over state-of-the-art co-located baselines, all while achieving 100% SLO attainment.</p></details> | 17 pages, 15 figures |
| **[Aragog: Just-in-Time Model Routing for Scalable Serving of Agentic Workflows](https://arxiv.org/abs/2511.20975v2)** | 2025-12-12 | <details><summary>Show</summary><p>Agentic workflows have emerged as a powerful paradigm for solving complex, multi-stage tasks, but serving them at scale is computationally expensive given the many LLM inferences that each request must pass through. Configuration selection, or the cost-aware assignment of workflow agents to specific LLMs, can reduce these costs, but existing approaches bind configuration decisions before request execution, making them ill-suited for the heterogeneous and lengthy execution of workflows. Specifically, system loads can fluctuate rapidly and substantially during a request's lifetime, causing fixed configurations to quickly become suboptimal. We present Aragog, a system that progressively adapts a request's configuration throughout its execution to match runtime dynamics. To make this practical despite the massive space of workflow configurations, Aragog decouples the problem into two core elements -- a one-time routing step that identifies all accuracy-preserving configurations, and a cheap per-stage scheduler that selects among them using up-to-date system observations -- and introduces novel strategies to accelerate each. Across diverse workflows and model families, Aragog increases maximum serving throughput by 50.0--217.0\% and reduces median latency by 32.5--78.9\% at peak request rates, while maintaining accuracy comparable to the most expensive configurations.</p></details> |  |
| **[SSSP-Del: Fully Dynamic Distributed Algorithm for Single-Source Shortest Path](https://arxiv.org/abs/2508.14319v2)** | 2025-12-12 | <details><summary>Show</summary><p>Modern graphs are both large and dynamic, presenting significant challenges for fundamental queries, such as the Single-Source Shortest Path (SSSP) problem. Naively recomputing the SSSP tree after each topology change is prohibitively expensive, causing on-demand computation to suffer from high latency. Existing dynamic SSSP algorithms often cannot simultaneously handle both edge additions and deletions, operate in distributed memory, and provide low-latency query results. To address these challenges, this paper presents SSSP-Del, a new vertex-centric, asynchronous, and fully distributed algorithm for dynamic SSSP. Operating in a shared-nothing architecture, our algorithm processes streams of both edge insertions and deletions. We conduct a comprehensive evaluation on large real-world and synthetic graphs with millions of vertices, and provide a thorough analysis by evaluating result latency, solution stability, and throughput.</p></details> |  |
| **[GSplit: Scaling Graph Neural Network Training on Large Graphs via Split-Parallelism](https://arxiv.org/abs/2303.13775v3)** | 2025-12-12 | <details><summary>Show</summary><p>Graph neural networks (GNNs), an emerging class of machine learning models for graphs, have gained popularity for their superior performance in various graph analytical tasks. Mini-batch training is commonly used to train GNNs on large graphs, and data parallelism is the standard approach to scale mini-batch training across multiple GPUs. Data parallel approaches contain redundant work as subgraphs sampled by different GPUs contain significant overlap. To address this issue, we introduce a hybrid parallel mini-batch training paradigm called split parallelism. Split parallelism avoids redundant work by splitting the sampling, loading, and training of each mini-batch across multiple GPUs. Split parallelism, however, introduces communication overheads that can be more than the savings from removing redundant work. We further present a lightweight partitioning algorithm that probabilistically minimizes these overheads. We implement split parallelism in GSplit and show that it outperforms state-of-the-art mini-batch training systems like DGL, Quiver, and $P^3$.</p></details> | <details><summary>Publi...</summary><p>Published at MLSys 2025. OpenReview: https://openreview.net/forum?id=cTOx1YTBgh</p></details> |
| **[Theoretical Foundations of GPU-Native Compilation for Rapid Code Iteration](https://arxiv.org/abs/2512.11200v1)** | 2025-12-12 | <details><summary>Show</summary><p>Current AI code generation systems suffer from significant latency bottlenecks due to CPU-GPU data transfers during compilation, execution, and testing phases. We establish theoretical foundations for three complementary approaches to GPU-native compilation that eliminate these transfers: (1) parallel traditional compilation adapted for GPU execution, (2) neural compilation using learned sequence-to-sequence translation with probabilistic verification, and (3) hybrid architectures combining both strategies. We derive latency and energy bounds demonstrating potential speedups of 10-100x for code iteration cycles. Our analysis shows that traditional GPU compilation provides 2-5x improvements through transfer elimination, neural compilation achieves 10-100x speedups via massive parallelism, and hybrid approaches offer practical deployment paths with guaranteed correctness. We formalize the probabilistic verification framework that enables trading compilation accuracy for parallel exploration, and discuss implications for self-improving AI systems and future analog computing substrates.</p></details> | 9 pages , 2 tables |
| **[An Efficient and Adaptive Watermark Detection System with Tile-based Error Correction](https://arxiv.org/abs/2509.02447v2)** | 2025-12-12 | <details><summary>Show</summary><p>Efficient and reliable detection of generated images is critical for the responsible deployment of generative models. Existing approaches primarily focus on improving detection accuracy and robustness under various image transformations and adversarial manipulations, yet they largely overlook the efficiency challenges of watermark detection across large-scale image collections. To address this gap, we propose QRMark, an efficient and adaptive end-to-end method for detecting embedded image watermarks. The core idea of QRMark is to combine QR Code-inspired error correction with tailored tiling techniques to improve detection efficiency while preserving accuracy and robustness. At the algorithmic level, QRMark employs a Reed-Solomon error correction mechanism to mitigate the accuracy degradation introduced by tiling. At the system level, QRMark implements a resource-aware multi-channel horizontal fusion policy that adaptively assigns more streams to GPU-intensive stages of the detection pipeline. It further employs a tile-based workload interleaving strategy to overlap data-loading overhead with computation and schedules kernels across stages to maximize efficiency. End-to-end evaluations show that QRMark achieves an average 2.43x inference speedup over the sequential baseline.</p></details> |  |
| **[KVSwap: Disk-aware KV Cache Offloading for Long-Context On-device Inference](https://arxiv.org/abs/2511.11907v2)** | 2025-12-11 | <details><summary>Show</summary><p>Language models (LMs) underpin emerging mobile and embedded AI applications like meeting and video summarization and document analysis, which often require processing multiple long-context inputs. Running an LM locally on-device improves privacy, enables offline use, and reduces cost, but long-context inference quickly hits a \emph{memory capacity wall} as the key-value (KV) cache grows linearly with context length and batch size. Existing KV-cache offloading schemes are designed to transfer cache data from GPU memory to CPU memory; however, they are not suitable for embedded and mobile systems, where the CPU and GPU (or NPU) typically share a unified memory and the non-volatile secondary storage (disk) offers limited I/O bandwidth. We present KVSwap, a software framework tailored for local devices that achieves high memory efficiency while effectively leveraging disk storage. KVSwap stores the full cache on disk, uses highly compact in-memory metadata to predict which entries to preload, overlaps computation with hardware-aware disk access, and orchestrates read patterns to match storage device characteristics. Our evaluation shows that across representative LMs and storage types, KVSwap delivers higher throughput under tight memory budgets while maintaining generation quality over existing KV cache offloading schemes.</p></details> |  |

